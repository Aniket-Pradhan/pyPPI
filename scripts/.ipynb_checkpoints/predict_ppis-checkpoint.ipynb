{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "\"\"\"\n",
    "This script runs classifier training over the entire training data and then\n",
    "output predictions over the interactome.\n",
    "\n",
    "Usage:\n",
    "  predict_ppis.py [--interpro] [--pfam] [--mf] [--cc] [--bp]\n",
    "                  [--use_cache] [--retrain] [--induce] [--verbose]\n",
    "                  [--model=M] [--n_jobs=J] [--n_splits=S] [--n_iterations=I]\n",
    "                  [--input=FILE] [--output=FILE] [--directory=DIR]\n",
    "  predict_ppis.py -h | --help\n",
    "\n",
    "Options:\n",
    "  -h --help     Show this screen.\n",
    "  --interpro    Use interpro domains in features.\n",
    "  --pfam        Use Pfam domains in features.\n",
    "  --mf          Use Molecular Function Gene Ontology in features.\n",
    "  --cc          Use Cellular Compartment Gene Ontology in features.\n",
    "  --bp          Use Biological Process Gene Ontology in features.\n",
    "  --induce      Use ULCA inducer over Gene Ontology.\n",
    "  --verbose     Print intermediate output for debugging.\n",
    "  --binary      Use binary feature encoding instead of ternary.\n",
    "  --use_cache   Use cached features if available.\n",
    "  --retrain     Re-train classifier instead of loading previous version. If\n",
    "                using a previous version, you must use the same selection of\n",
    "                features along with the same induce setting.\n",
    "  --model=M         A binary classifier from Scikit-Learn implementing fit,\n",
    "                    predict and predict_proba [default: LogisticRegression].\n",
    "                    Ignored if using 'retrain'.\n",
    "  --n_jobs=J        Number of processes to run in parallel [default: 1]\n",
    "  --n_splits=S      Number of cross-validation splits used during randomized\n",
    "                    grid search [default: 5]\n",
    "  --n_iterations=I  Number of randomized grid search iterations [default: 60]\n",
    "  --input=FILE      Uniprot edge-list, with a path directory that absolute or\n",
    "                    relative to this script. Entries must be tab separated with\n",
    "                    header columns 'source' and 'target'. [default: 'default']\n",
    "  --output=FILE     Output file name [default: predictions.tsv]\n",
    "  --directory=DIR   Absolute or relative output directory [default: ./results/]\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "from pyppi.base import parse_args, su_make_dir\n",
    "from pyppi.base import P1, P2, G1, G2\n",
    "from pyppi.data import load_network_from_path, load_ptm_labels\n",
    "from pyppi.data import full_training_network_path, generic_io\n",
    "from pyppi.data import interactome_network_path, classifier_path\n",
    "\n",
    "from pyppi.models import make_classifier, get_parameter_distribution_for_model\n",
    "\n",
    "from pyppi.data_mining.features import AnnotationExtractor\n",
    "from pyppi.data_mining.uniprot import UniProt, get_active_instance\n",
    "from pyppi.data_mining.tools import xy_from_interaction_frame\n",
    "from pyppi.data_mining.generic import edgelist_func, generic_to_dataframe\n",
    "from pyppi.data_mining.tools import map_network_accessions\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "logging.captureWarnings(False)\n",
    "logging.basicConfig(\n",
    "    format='[%(asctime)s] %(levelname)s: %(message)s',\n",
    "    datefmt='%m-%d-%Y %I:%M:%S',\n",
    "    level=logging.DEBUG,\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "args = dict(\n",
    "    n_jobs=3,\n",
    "    n_splits=2,\n",
    "    rcv_iter=3,\n",
    "    induce=True,\n",
    "    verbose=True,\n",
    "    selection = [\n",
    "        UniProt.data_types().GO_MF.value,\n",
    "        UniProt.data_types().GO_BP.value,\n",
    "        UniProt.data_types().GO_CC.value,\n",
    "        UniProt.data_types().INTERPRO.value,\n",
    "        UniProt.data_types().PFAM.value\n",
    "    ],\n",
    "    model='LogisticRegression',\n",
    "    use_cache=True,\n",
    "    output='predictions.tsv',\n",
    "    input='test_network.tsv',\n",
    "    direc='./results/',\n",
    "    retrain=True,\n",
    "    use_binary=False\n",
    ")\n",
    "n_jobs = args['n_jobs']\n",
    "n_splits = args['n_splits']\n",
    "rcv_iter = args['rcv_iter']\n",
    "induce = args['induce']\n",
    "verbose = args['verbose']\n",
    "selection = args['selection']\n",
    "model = args['model']\n",
    "use_feature_cache = args['use_cache']\n",
    "out_file = args['output']\n",
    "input_file = args['input']\n",
    "direc = args['direc']\n",
    "retrain = args['retrain']\n",
    "use_binary = args['use_binary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the folder for each experiment run named after the current time\n",
    "# -------------------------------------------------------------------- #\n",
    "folder = datetime.now().strftime(\"pred_%y-%m-%d_%H-%M-%S\")\n",
    "direc = \"{}/{}/\".format(direc, folder)\n",
    "su_make_dir(direc)\n",
    "json.dump(args, fp=open(\"{}/settings.json\".format(direc), 'w'), indent=4,\n",
    "          sort_keys=True)\n",
    "out_file = open(\"{}/{}\".format(direc, out_file), \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11-22-2017 12:00:12] INFO: Loading feature data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First time loading on UniProt instance. Make take a few moments\n",
      "Warning: Loading dat files, may take a few minutes.\n"
     ]
    }
   ],
   "source": [
    "# Load features from feature cache, or create an empty annotation extrator\n",
    "# ------------------------------------------------------------------------ #\n",
    "logger.info(\"Loading feature data...\")\n",
    "uniprot = get_active_instance(verbose=verbose)\n",
    "data_types = UniProt.data_types()\n",
    "labels = load_ptm_labels()\n",
    "training_ae = AnnotationExtractor(\n",
    "    induce=induce,\n",
    "    selection=selection,\n",
    "    n_jobs=n_jobs,\n",
    "    verbose=verbose,\n",
    "    cache=use_feature_cache\n",
    ")\n",
    "testing_ae = AnnotationExtractor(\n",
    "    induce=induce,\n",
    "    selection=selection,\n",
    "    n_jobs=n_jobs,\n",
    "    verbose=verbose,\n",
    "    cache=use_feature_cache\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11-22-2017 12:04:04] INFO: Loading custom ppi data...\n"
     ]
    }
   ],
   "source": [
    "# Get the input edge-list ready\n",
    "# -------------------------------------------------------------------- #\n",
    "if input_file == 'default':\n",
    "    logger.info(\"Loading interactome data...\")\n",
    "    testing = load_network_from_path(interactome_network_path)\n",
    "else:\n",
    "    logger.info(\"Loading custom ppi data...\")\n",
    "    testing = generic_to_dataframe(\n",
    "        f_input=generic_io(input_file),\n",
    "        parsing_func=edgelist_func,\n",
    "        drop_nan=True,\n",
    "        allow_self_edges=True,\n",
    "        allow_duplicates=True\n",
    "    )\n",
    "    sources = set(p for p in testing.source.values)\n",
    "    targets = set(p for p in testing.target.values)\n",
    "    accessions = list(sources | targets)\n",
    "    accession_mapping = uniprot.batch_map(accessions)\n",
    "    testing = map_network_accessions(\n",
    "        interactions=testing, accession_map=accession_mapping,\n",
    "        drop_nan=True, allow_self_edges=True,\n",
    "        allow_duplicates=False, min_counts=None, merge=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the features into X, and multilabel y indicator format\n",
    "# -------------------------------------------------------------------- #\n",
    "logger.info(\"Preparing training and testing data...\")\n",
    "training = load_network_from_path(full_training_network_path)\n",
    "X_train_ppis, y_train = xy_from_interaction_frame(training)\n",
    "X_test_ppis, _ = xy_from_interaction_frame(testing)\n",
    "\n",
    "if not use_feature_cache:\n",
    "    logger.info(\"Computing feature for input network.\")\n",
    "    testing_ae.fit(X_test_ppis)\n",
    "    X_train = training_ae.transform(X_train_ppis)\n",
    "    X_test = testing_ae.transform(X_test_ppis)\n",
    "else:\n",
    "    try:\n",
    "        X_train = training_ae.transform(X_train_ppis)\n",
    "    except ValueError:\n",
    "        logger.info(\"Found new training PPIs. Re-computing feature cache.\")\n",
    "        X_train = training_ae.fit_transform(X_train_ppis)\n",
    "        \n",
    "    try:\n",
    "        X_test = testing_ae.transform(X_test_ppis)\n",
    "    except ValueError:\n",
    "        logger.info(\"Found new testing PPIs. Re-computing feature cache.\")\n",
    "        X_test = testing_ae.fit_transform(X_test_ppis)\n",
    "\n",
    "# Get all annotations used during training\n",
    "# -------------------------------------------------------------------- #\n",
    "training_go = set([\n",
    "    x.strip() for xs in X_train\n",
    "    for x in xs.split(',')\n",
    "    if 'go' in x.strip().lower()\n",
    "])\n",
    "training_pfam = set([\n",
    "    x.strip() for xs in X_train\n",
    "    for x in xs.split(',')\n",
    "    if 'pf' in x.strip().lower()\n",
    "])\n",
    "training_ipr = set([\n",
    "    x.strip() for xs in X_train\n",
    "    for x in xs.split(',')\n",
    "    if 'ipr' in x.strip().lower()\n",
    "])\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=sorted(labels))\n",
    "mlb.fit(y_train)\n",
    "y_train = mlb.transform(y_train)\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True if use_binary else False)\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make the estimators and BR classifier\n",
    "# -------------------------------------------------------------------- #\n",
    "if retrain or not os.path.isfile(classifier_path):\n",
    "    logger.info(\"Making classifier...\")\n",
    "    params = get_parameter_distribution_for_model(model)\n",
    "    random_cv = RandomizedSearchCV(\n",
    "        cv=n_splits,\n",
    "        n_iter=rcv_iter,\n",
    "        n_jobs=n_jobs,\n",
    "        refit=True, \n",
    "        random_state=0,\n",
    "        scoring='f1', \n",
    "        error_score=0,\n",
    "        param_distributions=params,\n",
    "        estimator=make_classifier(model)\n",
    "    )\n",
    "    clf = OneVsRestClassifier(estimator=random_cv, n_jobs=1)\n",
    "\n",
    "    # Fit the complete training data and make predictions.\n",
    "    logging.info(\"Fitting data...\")\n",
    "    clf.fit(X_train, y_train)\n",
    "    joblib.dump(clf, classifier_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads a previously (or recently trained) classifier from disk\n",
    "# and then performs the predictions on the new dataset.\n",
    "# -------------------------------------------------------------------- #\n",
    "logging.info(\"Making predictions...\")\n",
    "clf = joblib.load(classifier_path)\n",
    "predictions = clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the predictions to a tsv file\n",
    "# -------------------------------------------------------------------- #\n",
    "logging.info(\"Writing results to file...\")\n",
    "header = \"{p1}\\t{p2}\\t{g1}\\t{g2}\\t{classes}\\tsum\\tusability_go\" \\\n",
    "         \"\\tusability_pf\\tusability_ipr\\n\".format(\n",
    "    p1=P1, p2=P2, g1=G1, g2=G2, classes='\\t'.join(sorted(mlb.classes_))\n",
    ")\n",
    "out_file.write(header)\n",
    "acc = testing_ae.accession_vocabulary[UniProt.accession_column()]\n",
    "genes = testing_ae.accession_vocabulary[UniProt.data_types().GENE.value]\n",
    "accession_gene_map = {a: g for (a, g) in zip(acc, genes)}\n",
    "for (s, t), p_vec in zip(X_test_ppis, predictions):\n",
    "    p_vec = [p for _, p in sorted(zip(mlb.classes_, p_vec))]\n",
    "    g1 = accession_gene_map.get(s, ['-'])[0] or '-'\n",
    "    g2 = accession_gene_map.get(t, ['-'])[0] or '-'\n",
    "\n",
    "    # Compute the usability of each of the annotation sets\n",
    "    annots = testing_ae.transform([(s, t)])\n",
    "    go = set([\n",
    "        x.strip() for xs in annots\n",
    "        for x in xs.split(',')\n",
    "        if 'go' in x.strip().lower()\n",
    "    ])\n",
    "    pf = set([\n",
    "        x.strip() for xs in annots\n",
    "        for x in xs.split(',')\n",
    "        if 'pf' in x.strip().lower()\n",
    "    ])\n",
    "    ipr = set([\n",
    "        x.strip() for xs in annots\n",
    "        for x in xs.split(',')\n",
    "        if 'ipr' in x.strip().lower()\n",
    "    ])\n",
    "    usability_go = len(go & training_go) / len(go)\n",
    "    usability_pf = len(pf & training_pfam) / len(pf)\n",
    "    usability_ipr = len(ipr & training_ipr) / len(ipr)\n",
    "\n",
    "    sum_pr = sum(p_vec)\n",
    "    line = \"{s}\\t{t}\\t{g1}\\t{g2}\\t{classes}\\t{sum_pr}\\t{usability_go}\" \\\n",
    "           \"\\t{usability_pf}\\t{usability_ipr}\\n\".format(\n",
    "            s=s, t=t, g1=g1, g2=g2, sum_pr=sum_pr,\n",
    "            classes='\\t'.join(['%.4f' % p for p in p_vec]),\n",
    "            usability_go=usability_go,\n",
    "            usability_pf=usability_pf,\n",
    "            usability_ipr=usability_ipr)\n",
    "    out_file.write(line)\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyppi]",
   "language": "python",
   "name": "conda-env-pyppi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
