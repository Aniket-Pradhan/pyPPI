{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'joblib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9c62ebf66683>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjoblib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelayed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdocopt\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdocopt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'joblib'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from numpy.random import RandomState\n",
    "from joblib import Parallel, delayed\n",
    "from datetime import datetime\n",
    "from docopt import docopt\n",
    "\n",
    "from pyppi.base import parse_args, su_make_dir, chunk_list\n",
    "from pyppi.base import P1, P2, G1, G2, SOURCE, TARGET, PUBMED, EXPERIMENT_TYPE\n",
    "from pyppi.base.logging import create_logger\n",
    "from pyppi.data import load_network_from_path, load_ptm_labels\n",
    "from pyppi.data import full_training_network_path, generic_io\n",
    "from pyppi.data import interactome_network_path, classifier_path\n",
    "from pyppi.data import default_db_path\n",
    "\n",
    "from pyppi.models import make_classifier, get_parameter_distribution_for_model\n",
    "\n",
    "from pyppi.database import make_session\n",
    "from pyppi.database.models import Interaction\n",
    "from pyppi.database.managers import InteractionManager, ProteinManager\n",
    "from pyppi.database.managers import format_interactions_for_sklearn\n",
    "from pyppi.database.utilities import update_interaction\n",
    "\n",
    "from pyppi.data_mining.tools import xy_from_interaction_frame\n",
    "from pyppi.data_mining.generic import edgelist_func, generic_to_dataframe\n",
    "from pyppi.data_mining.tools import map_network_accessions\n",
    "from pyppi.data_mining.uniprot import batch_map\n",
    "from pyppi.data_mining.features import compute_interaction_features\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "MAX_SEED = 1000000\n",
    "RANDOM_STATE = 42\n",
    "logger = create_logger(\"scripts\", logging.INFO)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # args = parse_args(docopt(__doc__))\n",
    "    n_jobs = 4  # args['n_jobs']\n",
    "    n_splits = 3  # args['n_splits']\n",
    "    rcv_iter = 10  # args['n_iterations']\n",
    "    induce = False  # args['induce']\n",
    "    verbose = False  # args['verbose']\n",
    "    selection = ['pfam']  # args['selection']\n",
    "    model = 'LogisticRegression'  # args['model']\n",
    "    out_file = 'predictions.tsv'  # args['output']\n",
    "    input_file = None  # args['input']\n",
    "    direc = './results/'  # args['directory']\n",
    "\n",
    "    # Set up the folder for each experiment run named after the current time\n",
    "    # -------------------------------------------------------------------- #\n",
    "    folder = datetime.now().strftime(\"pred_%y-%m-%d_%H-%M-%S\")\n",
    "    direc = \"{}/{}/\".format(direc, folder)\n",
    "    su_make_dir(direc)\n",
    "    json.dump(\n",
    "        args, fp=open(\"{}/settings.json\".format(direc), 'w'),\n",
    "        indent=4, sort_keys=True\n",
    "    )\n",
    "\n",
    "    logger.info(\"Starting new database session.\")\n",
    "    session = make_session(db_path=default_db_path)\n",
    "    i_manager = InteractionManager(verbose=verbose, match_taxon_id=9606)\n",
    "    p_manager = ProteinManager(verbose=verbose, match_taxon_id=9606)\n",
    "    protein_map = p_manager.uniprotid_entry_map(session)\n",
    "\n",
    "    # Get the input edge-list ready\n",
    "    # -------------------------------------------------------------------- #\n",
    "    labels = i_manager.training_labels(session, include_holdout=True)\n",
    "    training = i_manager.training_interactions(\n",
    "        session, keep_holdout=True\n",
    "    )\n",
    "\n",
    "    if input_file == None:\n",
    "        logger.info(\"Loading interactome data.\")\n",
    "        testing = i_manager.interactome_interactions(\n",
    "            session=session,\n",
    "            keep_holdout=True,\n",
    "            keep_training=True\n",
    "        )\n",
    "    else:\n",
    "        logger.info(\"Loading custom ppi data.\")\n",
    "        testing = generic_to_dataframe(\n",
    "            f_input=generic_io(input_file),\n",
    "            parsing_func=edgelist_func,\n",
    "            drop_nan=True,\n",
    "            allow_self_edges=True,\n",
    "            allow_duplicates=True\n",
    "        )\n",
    "        sources = set(p for p in testing.source.values)\n",
    "        targets = set(p for p in testing.target.values)\n",
    "        accessions = list(sources | targets)\n",
    "        accession_mapping = batch_map(\n",
    "            session=session,\n",
    "            accessions=accessions,\n",
    "            keep_unreviewed=True,\n",
    "            match_taxon_id=9606,\n",
    "            allow_download=True\n",
    "        )\n",
    "        testing_network = map_network_accessions(\n",
    "            interactions=testing, accession_map=accession_mapping,\n",
    "            drop_nan=True, allow_self_edges=True,\n",
    "            allow_duplicates=False, min_counts=None, merge=False\n",
    "        )\n",
    "\n",
    "        # Compute features for new ppis\n",
    "        testing = []\n",
    "        feature_map = {}\n",
    "        ppis = [\n",
    "            (protein_map[a], protein_map[b])\n",
    "            for (a, b) in zip(testing_network[SOURCE], testing_network[TARGET])\n",
    "            if i_manager.get_by_source_target(session, a, b) is None\n",
    "        ]\n",
    "\n",
    "        logger.info(\"Computing features.\")\n",
    "        features = Parallel(n_jobs=n_jobs, backend=\"multiprocessing\")(\n",
    "            delayed(compute_interaction_features)(source, target)\n",
    "            for (source, target) in ppis\n",
    "        )\n",
    "        for (source, target), features in zip(ppis, features):\n",
    "            feature_map[(source.uniprot_id, target.uniprot_id)] = features\n",
    "\n",
    "        existing_interactions = {}\n",
    "        for interaction in session.query(Interaction).all():\n",
    "            a = p_manager.get_by_id(session, id=interaction.source)\n",
    "            b = p_manager.get_by_id(session, id=interaction.target)\n",
    "            existing_interactions[(a.uniprot_id, b.uniprot_id)] = interaction\n",
    "\n",
    "        for (a, b) in zip(testing_network[SOURCE], testing_network[TARGET]):\n",
    "            class_kwargs = feature_map[(a, b)]\n",
    "            class_kwargs[\"source\"] = protein_map[a]\n",
    "            class_kwargs[\"target\"] = protein_map[b]\n",
    "            class_kwargs[\"label\"] = None\n",
    "            class_kwargs[\"is_training\"] = False\n",
    "            class_kwargs[\"is_holdout\"] = False\n",
    "            class_kwargs[\"is_interactome\"] = False\n",
    "            entry = update_interaction(\n",
    "                session=session,\n",
    "                commit=False,\n",
    "                psimi_ls=[],\n",
    "                pmid_ls=[],\n",
    "                replace_fields=False,\n",
    "                override_boolean=False,\n",
    "                create_if_not_found=True,\n",
    "                match_taxon_id=9606,\n",
    "                verbose=False,\n",
    "                update_features=False,\n",
    "                existing_interactions=existing_interactions,\n",
    "                **class_kwargs\n",
    "            )\n",
    "            existing_interactions[(a, b)] = entry\n",
    "            testing.append(entry)\n",
    "        session.commit()\n",
    "\n",
    "    # Get the features into X, and multilabel y indicator format\n",
    "    # -------------------------------------------------------------------- #\n",
    "    logger.info(\"Preparing training and testing data.\")\n",
    "    X_train, y_train = format_interactions_for_sklearn(training, selection)\n",
    "    X_test, _ = format_interactions_for_sklearn(testing, selection)\n",
    "\n",
    "    logger.info(\"Computing usable feature proportions in testing samples.\")\n",
    "\n",
    "    def separate_features(row):\n",
    "        features = row[0].upper().split(',')\n",
    "        interpro = set(term for term in features if 'IPR' in term)\n",
    "        go = set(term for term in features if 'GO:' in term)\n",
    "        pfam = set(term for term in features if 'PF' in term)\n",
    "        return (go, interpro, pfam)\n",
    "\n",
    "    def compute_proportions_shared(row):\n",
    "        go, ipr, pf = row\n",
    "        try:\n",
    "            go_prop = len(go & go_training) / len(go)\n",
    "        except ZeroDivisionError:\n",
    "            go_prop = 0\n",
    "        try:\n",
    "            ipr_prop = len(ipr & ipr_training) / len(ipr)\n",
    "        except ZeroDivisionError:\n",
    "            ipr_prop = 0\n",
    "        try:\n",
    "            pf_prop = len(pf & pf_training) / len(pf)\n",
    "        except ZeroDivisionError:\n",
    "            pf_prop = 0\n",
    "        return go_prop, ipr_prop, pf_prop\n",
    "\n",
    "    X_train_split_features = np.apply_along_axis(\n",
    "        separate_features, axis=1, arr=X_train.reshape((X_train.shape[0], 1))\n",
    "    )\n",
    "    go_training = set()\n",
    "    ipr_training = set()\n",
    "    pf_training = set()\n",
    "    for (go, ipr, pf) in X_train_split_features:\n",
    "        go_training |= go\n",
    "        ipr_training |= ipr\n",
    "        pf_training |= pf\n",
    "\n",
    "    X_test_split_features = np.apply_along_axis(\n",
    "        separate_features, axis=1, arr=X_test.reshape((X_test.shape[0], 1))\n",
    "    )\n",
    "    X_test_useable_props = np.apply_along_axis(\n",
    "        compute_proportions_shared, axis=1, arr=X_test_split_features\n",
    "    )\n",
    "\n",
    "    mlb = MultiLabelBinarizer(classes=sorted(labels))\n",
    "    mlb.fit(y_train)\n",
    "    y_train = mlb.transform(y_train)\n",
    "\n",
    "    # X_train = vectorizer.fit_transform(X_train)\n",
    "    # X_test = vectorizer.transform(X_test)\n",
    "\n",
    "    # Make the estimators and BR classifier\n",
    "    # -------------------------------------------------------------------- #\n",
    "    rng = RandomState(seed=RANDOM_STATE)\n",
    "    params = get_parameter_distribution_for_model(model)\n",
    "    for key in params.keys():\n",
    "        value = params.pop(key)\n",
    "        params['estimator__{}'.format(key)] = value\n",
    "    params['vectorizer__binary'] = [False, True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
