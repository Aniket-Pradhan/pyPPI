{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-15 10:56:06,169 scripts      INFO     Clearing existing database tables.\n",
      "2018-02-15 10:56:06,229 scripts      INFO     Parsing UniProt and PSI-MI into database.\n",
      "2018-02-15 10:58:59,593 scripts      INFO     Starting new database session.\n",
      "2018-02-15 10:58:59,598 scripts      INFO     Building KEGG interactions.\n",
      "2018-02-15 10:59:49,996 pyppi        INFO     Warning: More that one reviewed acc found for hsa:102724560: ['P0DN79', 'P35520']\n",
      "2018-02-15 10:59:51,661 pyppi        INFO     Warning: More that one reviewed acc found for hsa:1442: ['P0DML2', 'P0DML3']\n",
      "2018-02-15 10:59:53,232 pyppi        INFO     Warning: No reviewed acc found for hsa:504191.\n",
      "2018-02-15 10:59:53,234 pyppi        INFO     Warning: Could not map hsa:504191.\n",
      "2018-02-15 10:59:53,291 pyppi        INFO     Warning: No reviewed acc found for hsa:1056.\n",
      "2018-02-15 10:59:53,292 pyppi        INFO     Warning: Could not map hsa:1056.\n",
      "2018-02-15 10:59:55,448 pyppi        INFO     Warning: More that one reviewed acc found for hsa:808: ['P0DP23', 'P0DP24', 'P0DP25']\n",
      "2018-02-15 10:59:55,707 pyppi        INFO     Warning: No reviewed acc found for hsa:80319.\n",
      "2018-02-15 10:59:55,709 pyppi        INFO     Warning: Could not map hsa:80319.\n",
      "2018-02-15 10:59:57,108 pyppi        INFO     Warning: No reviewed acc found for hsa:107181291.\n",
      "2018-02-15 10:59:57,110 pyppi        INFO     Warning: Could not map hsa:107181291.\n",
      "2018-02-15 10:59:58,671 pyppi        INFO     Warning: More that one reviewed acc found for hsa:2778: ['O95467', 'P63092']\n",
      "2018-02-15 10:59:59,870 pyppi        INFO     Warning: More that one reviewed acc found for hsa:875: ['P0DN79', 'P35520']\n",
      "2018-02-15 10:59:59,984 pyppi        INFO     Warning: More that one reviewed acc found for hsa:388372: ['P13236', 'Q8NHW4']\n",
      "2018-02-15 11:00:01,325 pyppi        INFO     Warning: No reviewed acc found for hsa:256892.\n",
      "2018-02-15 11:00:01,327 pyppi        INFO     Warning: Could not map hsa:256892.\n",
      "2018-02-15 11:00:01,617 pyppi        INFO     Warning: No reviewed acc found for hsa:3810.\n",
      "2018-02-15 11:00:01,619 pyppi        INFO     Warning: Could not map hsa:3810.\n",
      "2018-02-15 11:00:02,066 pyppi        INFO     Warning: More that one reviewed acc found for hsa:721: ['P0C0L4', 'P0C0L5']\n",
      "2018-02-15 11:00:03,064 pyppi        INFO     Warning: No reviewed acc found for hsa:100526760.\n",
      "2018-02-15 11:00:03,065 pyppi        INFO     Warning: Could not map hsa:100526760.\n",
      "2018-02-15 11:00:03,318 pyppi        INFO     Warning: No reviewed acc found for hsa:442590.\n",
      "2018-02-15 11:00:03,320 pyppi        INFO     Warning: Could not map hsa:442590.\n",
      "2018-02-15 11:00:03,636 pyppi        INFO     Warning: No reviewed acc found for hsa:729597.\n",
      "2018-02-15 11:00:03,638 pyppi        INFO     Warning: Could not map hsa:729597.\n",
      "2018-02-15 11:00:06,199 pyppi        INFO     Warning: No reviewed acc found for hsa:3108.\n",
      "2018-02-15 11:00:06,200 pyppi        INFO     Warning: Could not map hsa:3108.\n",
      "2018-02-15 11:00:10,863 pyppi        INFO     Warning: More that one reviewed acc found for hsa:3304: ['P0DMV8', 'P0DMV9']\n",
      "2018-02-15 11:00:11,567 pyppi        INFO     Warning: More that one reviewed acc found for hsa:27113: ['Q96PG8', 'Q9BXH1']\n",
      "2018-02-15 11:00:11,712 pyppi        INFO     Warning: No reviewed acc found for hsa:100271927.\n",
      "2018-02-15 11:00:11,713 pyppi        INFO     Warning: Could not map hsa:100271927.\n",
      "2018-02-15 11:00:12,059 pyppi        INFO     Warning: No reviewed acc found for hsa:100132074.\n",
      "2018-02-15 11:00:12,060 pyppi        INFO     Warning: Could not map hsa:100132074.\n",
      "2018-02-15 11:00:13,977 pyppi        INFO     Warning: No reviewed acc found for hsa:387712.\n",
      "2018-02-15 11:00:13,980 pyppi        INFO     Warning: Could not map hsa:387712.\n",
      "2018-02-15 11:00:14,134 pyppi        INFO     Warning: No reviewed acc found for hsa:26687.\n",
      "2018-02-15 11:00:14,136 pyppi        INFO     Warning: Could not map hsa:26687.\n",
      "2018-02-15 11:00:17,095 pyppi        INFO     Warning: No reviewed acc found for hsa:26686.\n",
      "2018-02-15 11:00:17,097 pyppi        INFO     Warning: Could not map hsa:26686.\n",
      "2018-02-15 11:00:17,610 pyppi        INFO     Warning: No reviewed acc found for hsa:107987478.\n",
      "2018-02-15 11:00:17,612 pyppi        INFO     Warning: Could not map hsa:107987478.\n",
      "2018-02-15 11:00:17,843 pyppi        INFO     Warning: More that one reviewed acc found for hsa:801: ['P0DP23', 'P0DP24', 'P0DP25']\n",
      "2018-02-15 11:00:19,866 pyppi        INFO     Warning: More that one reviewed acc found for hsa:805: ['P0DP23', 'P0DP24', 'P0DP25']\n",
      "2018-02-15 11:00:21,635 pyppi        INFO     Warning: No reviewed acc found for hsa:441024.\n",
      "2018-02-15 11:00:21,637 pyppi        INFO     Warning: Could not map hsa:441024.\n",
      "2018-02-15 11:00:23,350 pyppi        INFO     Warning: More that one reviewed acc found for hsa:9378: ['P58400', 'Q9ULB1']\n",
      "2018-02-15 11:00:23,953 pyppi        INFO     Warning: More that one reviewed acc found for hsa:3303: ['P0DMV8', 'P0DMV9']\n",
      "2018-02-15 11:00:24,161 pyppi        INFO     Warning: No reviewed acc found for hsa:317749.\n",
      "2018-02-15 11:00:24,162 pyppi        INFO     Warning: Could not map hsa:317749.\n",
      "2018-02-15 11:00:27,477 pyppi        INFO     Warning: No reviewed acc found for hsa:100528021.\n",
      "2018-02-15 11:00:27,479 pyppi        INFO     Warning: Could not map hsa:100528021.\n",
      "2018-02-15 11:00:30,320 pyppi        INFO     Warning: No reviewed acc found for hsa:115653.\n",
      "2018-02-15 11:00:30,321 pyppi        INFO     Warning: Could not map hsa:115653.\n",
      "2018-02-15 11:00:34,875 scripts      INFO     Building HPRD interactions.\n",
      "2018-02-15 11:06:04,158 scripts      INFO     Building Interactome interactions.\n",
      "2018-02-15 11:06:55,147 scripts      INFO     Mapping to most recent uniprot accessions.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not download map from uniprot server.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-07d0388bcc32>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[0maccessions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccessions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m         \u001b[0mkeep_unreviewed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m         \u001b[0mmatch_taxon_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m9606\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m     )\n\u001b[0;32m    175\u001b[0m     \u001b[0msave_uniprot_accession_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccession_mapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pyppi\\lib\\site-packages\\pyppi-0.1-py3.5.egg\\pyppi\\data_mining\\uniprot.py\u001b[0m in \u001b[0;36mbatch_map\u001b[1;34m(session, accessions, fr, allow_download, keep_unreviewed, match_taxon_id)\u001b[0m\n\u001b[0;32m    177\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmapping\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Could not download map from uniprot server.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[0mpm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mProteinManager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatch_taxon_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Could not download map from uniprot server."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script runs classifier training over the entire training data and then\n",
    "output predictions over the interactome.\n",
    "\n",
    "Usage:\n",
    "  build_data.py [--clear_cache] [--n_jobs=J]\n",
    "  build_data.py -h | --help\n",
    "\n",
    "Options:\n",
    "  -h --help  Show this screen.\n",
    "  --n_jobs=J  Number of processes to run in parallel [default: 1]\n",
    "  --clear_cache  Delete previous bioservices KEGG/UniProt cache\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import logging\n",
    "from Bio import SwissProt\n",
    "from joblib import Parallel, delayed\n",
    "from docopt import docopt\n",
    "\n",
    "from pyppi.base import delete_cache\n",
    "from pyppi.base import parse_args, SOURCE, TARGET, LABEL\n",
    "from pyppi.base import PUBMED, EXPERIMENT_TYPE, NULL_VALUES\n",
    "\n",
    "from pyppi.data import bioplex_network_path, pina2_network_path\n",
    "from pyppi.data import bioplex_v4, pina2, innate_curated, innate_imported\n",
    "from pyppi.data import innate_i_network_path, innate_c_network_path\n",
    "from pyppi.data import interactome_network_path, full_training_network_path\n",
    "from pyppi.data import kegg_network_path, hprd_network_path\n",
    "from pyppi.data import save_uniprot_accession_map\n",
    "from pyppi.data import testing_network_path, training_network_path\n",
    "from pyppi.data import save_network_to_path\n",
    "from pyppi.data import save_ptm_labels\n",
    "from pyppi.data import default_db_path\n",
    "from pyppi.data import uniprot_sprot, uniprot_trembl\n",
    "\n",
    "from pyppi.database import make_session, begin_transaction, delete_database\n",
    "from pyppi.database.models import Protein, Interaction, Pubmed, Psimi\n",
    "from pyppi.database.utilities import generate_interaction_tuples\n",
    "from pyppi.database.utilities import update_interaction\n",
    "from pyppi.database.utilities import psimi_string_to_list, pmid_string_to_list\n",
    "from pyppi.database.managers import InteractionManager, ProteinManager\n",
    "\n",
    "from pyppi.data_mining.uniprot import parse_record_into_protein\n",
    "from pyppi.data_mining.uniprot import batch_map\n",
    "from pyppi.data_mining.generic import bioplex_func, mitab_func, pina_func\n",
    "from pyppi.data_mining.generic import generic_to_dataframe\n",
    "from pyppi.data_mining.hprd import hprd_to_dataframe\n",
    "from pyppi.data_mining.tools import process_interactions, make_interaction_frame\n",
    "from pyppi.data_mining.tools import remove_common_ppis, remove_labels\n",
    "from pyppi.data_mining.tools import map_network_accessions\n",
    "from pyppi.data_mining.kegg import download_pathway_ids, pathways_to_dataframe\n",
    "from pyppi.data_mining.ontology import get_active_instance\n",
    "from pyppi.data_mining.psimi import get_active_instance as load_mi_ontology\n",
    "from pyppi.data_mining.features import compute_interaction_features\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger = logging.getLogger(\"scripts\")\n",
    "    handler = logging.StreamHandler()\n",
    "    formatter = logging.Formatter(\n",
    "        '%(asctime)s %(name)-12s %(levelname)-8s %(message)s'\n",
    "    )\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logger.propagate = False\n",
    "\n",
    "#     args = docopt(__doc__)\n",
    "#     args = parse_args(args)\n",
    "    n_jobs = 15\n",
    "    clear_cache = False\n",
    "\n",
    "    i_manager = InteractionManager(verbose=True, match_taxon_id=9606)\n",
    "    p_manager = ProteinManager(verbose=True, match_taxon_id=9606)\n",
    "\n",
    "    # Setup the protein table in the database\n",
    "    # ----------------------------------------------------------------------- #\n",
    "    if clear_cache:\n",
    "        logger.info(\"Clearing Biopython/Bioservices cache.\")\n",
    "        delete_cache()\n",
    "\n",
    "    logger.info(\"Clearing existing database tables.\")\n",
    "    with begin_transaction(db_path=default_db_path) as session:\n",
    "        delete_database(session=session)\n",
    "\n",
    "    logger.info(\"Parsing UniProt and PSI-MI into database.\")\n",
    "    records = list(SwissProt.parse(uniprot_sprot())) + \\\n",
    "        list(SwissProt.parse(uniprot_trembl()))\n",
    "    with begin_transaction(db_path=default_db_path) as session:\n",
    "        proteins = [parse_record_into_protein(r) for r in records]\n",
    "        psimi_objects = []\n",
    "        mi_ont = load_mi_ontology()\n",
    "        for key, term in mi_ont.items():\n",
    "            obj = Psimi(accession=key, description=term.name)\n",
    "            psimi_objects.append(obj)\n",
    "\n",
    "        try:\n",
    "            session.add_all(proteins + psimi_objects)\n",
    "            session.commit()\n",
    "        except:\n",
    "            session.rollback()\n",
    "            raise\n",
    "\n",
    "    logger.info(\"Starting new database session.\")\n",
    "    session = make_session(db_path=default_db_path)\n",
    "\n",
    "    # Construct all the networks\n",
    "    # ----------------------------------------------------------------------- #\n",
    "    logger.info(\"Building KEGG interactions.\")\n",
    "    pathways = download_pathway_ids('hsa')\n",
    "    kegg = pathways_to_dataframe(\n",
    "        session=session,\n",
    "        pathway_ids=pathways,\n",
    "        map_to_uniprot=True,\n",
    "        drop_nan='default',\n",
    "        allow_self_edges=True,\n",
    "        allow_duplicates=False\n",
    "    )\n",
    "\n",
    "    logger.info(\"Building HPRD interactions.\")\n",
    "    hprd = hprd_to_dataframe(\n",
    "        session=session,\n",
    "        drop_nan='default',\n",
    "        allow_self_edges=True,\n",
    "        allow_duplicates=False\n",
    "    )\n",
    "\n",
    "    logger.info(\"Building Interactome interactions.\")\n",
    "    bioplex = generic_to_dataframe(\n",
    "        f_input=bioplex_v4(),\n",
    "        parsing_func=bioplex_func,\n",
    "        drop_nan=[SOURCE, TARGET],\n",
    "        allow_self_edges=True,\n",
    "        allow_duplicates=False\n",
    "    )\n",
    "\n",
    "    pina2 = generic_to_dataframe(\n",
    "        f_input=pina2(),\n",
    "        parsing_func=pina_func,\n",
    "        drop_nan=[SOURCE, TARGET],\n",
    "        allow_self_edges=True,\n",
    "        allow_duplicates=False\n",
    "    )\n",
    "\n",
    "    innate_c = generic_to_dataframe(\n",
    "        f_input=innate_curated(),\n",
    "        parsing_func=mitab_func,\n",
    "        drop_nan=[SOURCE, TARGET],\n",
    "        allow_self_edges=True,\n",
    "        allow_duplicates=False\n",
    "    )\n",
    "\n",
    "    innate_i = generic_to_dataframe(\n",
    "        f_input=innate_imported(),\n",
    "        parsing_func=mitab_func,\n",
    "        drop_nan=[SOURCE, TARGET],\n",
    "        allow_self_edges=True,\n",
    "        allow_duplicates=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-15 11:23:10,059 scripts      INFO     Mapping to most recent uniprot accessions.\n"
     ]
    }
   ],
   "source": [
    "    logger.info(\"Mapping to most recent uniprot accessions.\")\n",
    "    # Get a set of all the unique uniprot accessions\n",
    "    networks = [kegg, hprd, bioplex, pina2, innate_i, innate_c]\n",
    "    sources = set(p for df in networks for p in df.source.values)\n",
    "    targets = set(p for df in networks for p in df.target.values)\n",
    "    accessions = list(sources | targets)\n",
    "    accession_mapping = batch_map(\n",
    "        session=session,\n",
    "        allow_download=False,\n",
    "        accessions=accessions,\n",
    "        keep_unreviewed=True,\n",
    "        match_taxon_id=9606\n",
    "    )\n",
    "    save_uniprot_accession_map(accession_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    logger.info(\"Mapping each network to the most recent uniprot accessions.\")\n",
    "    kegg = map_network_accessions(\n",
    "        interactions=kegg, accession_map=accession_mapping,\n",
    "        drop_nan='default', allow_self_edges=True,\n",
    "        allow_duplicates=False, min_counts=None, merge=False\n",
    "    )\n",
    "\n",
    "    hprd = map_network_accessions(\n",
    "        interactions=hprd, accession_map=accession_mapping,\n",
    "        drop_nan='default', allow_self_edges=True,\n",
    "        allow_duplicates=False, min_counts=None, merge=False\n",
    "    )\n",
    "\n",
    "    pina2 = map_network_accessions(\n",
    "        interactions=pina2, accession_map=accession_mapping,\n",
    "        drop_nan=[SOURCE, TARGET], allow_self_edges=True,\n",
    "        allow_duplicates=False, min_counts=None, merge=False\n",
    "    )\n",
    "\n",
    "    bioplex = map_network_accessions(\n",
    "        interactions=bioplex, accession_map=accession_mapping,\n",
    "        drop_nan=[SOURCE, TARGET], allow_self_edges=True,\n",
    "        allow_duplicates=False, min_counts=None, merge=False\n",
    "    )\n",
    "\n",
    "    innate_c = map_network_accessions(\n",
    "        interactions=innate_c, accession_map=accession_mapping,\n",
    "        drop_nan=[SOURCE, TARGET], allow_self_edges=True,\n",
    "        allow_duplicates=False, min_counts=None, merge=False\n",
    "    )\n",
    "\n",
    "    innate_i = map_network_accessions(\n",
    "        interactions=innate_i, accession_map=accession_mapping,\n",
    "        drop_nan=[SOURCE, TARGET], allow_self_edges=True,\n",
    "        allow_duplicates=False, min_counts=None, merge=False\n",
    "    )\n",
    "    networks = [hprd, kegg, bioplex, pina2, innate_i, innate_c]\n",
    "\n",
    "    logger.info(\"Saving raw networks.\")\n",
    "    save_network_to_path(kegg, kegg_network_path)\n",
    "    save_network_to_path(hprd, hprd_network_path)\n",
    "    save_network_to_path(pina2, pina2_network_path)\n",
    "    save_network_to_path(bioplex, bioplex_network_path)\n",
    "    save_network_to_path(innate_i, innate_i_network_path)\n",
    "    save_network_to_path(innate_c, innate_c_network_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def remove_common_ppis(df_1, df_2):\n",
    "        \"\"\"\n",
    "        Collects all ppis which are common to df_1 and df_2 by looking at the\n",
    "        SOURCE and TARGET columns. Removes these common ppis from df_1 and df_2\n",
    "        and collects them into a new dataframe.\n",
    "\n",
    "        Note: Expected the SOURCE and TARGET columns to be pre-sorted, otherwise\n",
    "        this method will not detect permuted ppis (A, B)/(B, A).\n",
    "\n",
    "        :param df_1: \n",
    "            DataFrame with 'source', 'target' and 'label' columns.\n",
    "        :param df_2: \n",
    "            DataFrame with 'source', 'target' and 'label' columns.\n",
    "        :return: \n",
    "            tuple of DataFrames (df_1_unique, df_2_unique, common)\n",
    "        \"\"\"\n",
    "        ppis_df_1 = list(zip(df_1[SOURCE], df_1[TARGET]))\n",
    "        ppis_df_2 = list(zip(df_2[SOURCE], df_2[TARGET]))\n",
    "        common_ppis = set(ppis_df_1) & set(ppis_df_2)\n",
    "\n",
    "        unique_idx = set()\n",
    "        common_idx = set()\n",
    "        for idx, ppi in enumerate(ppis_df_1):\n",
    "            if ppi in common_ppis:\n",
    "                common_idx.add(idx)\n",
    "            else:\n",
    "                unique_idx.add(idx)\n",
    "        df_1_unique = df_1.loc[unique_idx, :]\n",
    "        df_1_common = df_1.loc[common_idx, :]\n",
    "\n",
    "        unique_idx = set()\n",
    "        common_idx = set()\n",
    "        for idx, ppi in enumerate(ppis_df_2):\n",
    "            if ppi in common_ppis:\n",
    "                common_idx.add(idx)\n",
    "            else:\n",
    "                unique_idx.add(idx)\n",
    "        df_2_unique = df_2.loc[unique_idx, :]\n",
    "        df_2_common = df_2.loc[common_idx, :]\n",
    "\n",
    "        common = pd.concat(\n",
    "            [df_1_common, df_2_common],\n",
    "            ignore_index=True\n",
    "        )\n",
    "        df_1_unique.reset_index(drop=True, inplace=True)\n",
    "        df_2_unique.reset_index(drop=True, inplace=True)\n",
    "        common.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        return df_1_unique, df_2_unique, common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-15 14:19:45,251 scripts      INFO     Building and saving processed networks.\n"
     ]
    }
   ],
   "source": [
    "    logger.info(\"Building and saving processed networks.\")\n",
    "    hprd_test_labels = ['dephosphorylation', 'phosphorylation']\n",
    "    hprd_train_labels = set(\n",
    "        [l for l in hprd[LABEL] if l not in hprd_test_labels]\n",
    "    )\n",
    "    train_hprd = remove_labels(hprd, hprd_test_labels)\n",
    "    testing = remove_labels(hprd, hprd_train_labels)\n",
    "    training = pd.concat([kegg, train_hprd], ignore_index=True).reset_index(\n",
    "        drop=True, inplace=False)\n",
    "\n",
    "    # Some ppis will be the same between training/testing sets but\n",
    "    # with different labels. Put all the ppis appearing in testing\n",
    "    # with a different label compared to the same instance in training\n",
    "    # into the training set. This way we can keep the testing and\n",
    "    # training sets completely disjoint.\n",
    "    training, testing, common = remove_common_ppis(\n",
    "        df_1=training,\n",
    "        df_2=testing\n",
    "    )\n",
    "    full_training = pd.concat(\n",
    "        [training, testing, common],\n",
    "        ignore_index=True\n",
    "    ).reset_index(\n",
    "        drop=True, inplace=False\n",
    "    )\n",
    "    \n",
    "    testing = process_interactions(\n",
    "        interactions=testing,\n",
    "        drop_nan='default', allow_duplicates=False, allow_self_edges=True,\n",
    "        exclude_labels=None, min_counts=5, merge=True\n",
    "    )\n",
    "    training = process_interactions(\n",
    "        interactions=training,\n",
    "        drop_nan='default', allow_duplicates=False, allow_self_edges=True,\n",
    "        exclude_labels=None, min_counts=5, merge=True\n",
    "    )\n",
    "    full_training = process_interactions(\n",
    "        interactions=full_training,\n",
    "        drop_nan='default', allow_duplicates=False, allow_self_edges=True,\n",
    "        exclude_labels=None, min_counts=5, merge=True\n",
    "    )\n",
    "    common = process_interactions(\n",
    "        interactions=common,\n",
    "        drop_nan='default', allow_duplicates=False, allow_self_edges=True,\n",
    "        exclude_labels=None, min_counts=None, merge=True\n",
    "    )\n",
    "\n",
    "    labels = list(full_training[LABEL])\n",
    "    ptm_labels = set(l for merged in labels for l in merged.split(','))\n",
    "    save_ptm_labels(ptm_labels)\n",
    "\n",
    "    interactome_networks = [bioplex, pina2, innate_i, innate_c]\n",
    "    interactome = pd.concat(interactome_networks, ignore_index=True)\n",
    "    interactome = process_interactions(\n",
    "        interactions=interactome, drop_nan=[SOURCE, TARGET],\n",
    "        allow_duplicates=False, allow_self_edges=True,\n",
    "        exclude_labels=None, min_counts=None, merge=True\n",
    "    )\n",
    "    save_network_to_path(interactome, interactome_network_path)\n",
    "    save_network_to_path(training, training_network_path)\n",
    "    save_network_to_path(testing, testing_network_path)\n",
    "    save_network_to_path(full_training, full_training_network_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-15 11:36:21,283 scripts      INFO     Saving Interaction records to database.\n",
      "2018-02-15 11:36:28,088 scripts      INFO     Computing features.\n"
     ]
    }
   ],
   "source": [
    "    logger.info(\"Saving Interaction records to database.\")\n",
    "    protein_map = p_manager.uniprotid_entry_map(session)\n",
    "    ppis = [\n",
    "        (protein_map[a], protein_map[b])\n",
    "        for network in [full_training, interactome]\n",
    "        for (a, b) in zip(network[SOURCE], network[TARGET])\n",
    "    ]\n",
    "\n",
    "    feature_map = {}\n",
    "    logger.info(\"Computing features.\")\n",
    "    features_ls = Parallel(n_jobs=n_jobs, backend='multiprocessing')(\n",
    "        delayed(compute_interaction_features)(source, target)\n",
    "        for (source, target) in ppis\n",
    "    )\n",
    "    for (source, target), features in zip(ppis, features_ls):\n",
    "        feature_map[(source.uniprot_id, target.uniprot_id)] = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-15 12:01:11,250 scripts      INFO     Updating Pubmed/PSI-MI database entries.\n"
     ]
    }
   ],
   "source": [
    "    # Create and save all the psimi and pubmed objects if they don't already\n",
    "    # exist in the database.\n",
    "    logger.info(\"Updating Pubmed/PSI-MI database entries.\")\n",
    "    objects = []\n",
    "    mi_ont = load_mi_ontology()\n",
    "    networks = [full_training, interactome]\n",
    "    pmids = set([\n",
    "        p for ls in pd.concat(networks, ignore_index=True)[PUBMED]\n",
    "        for p in str(ls).split(',')\n",
    "        if str(p) not in NULL_VALUES\n",
    "    ])\n",
    "    psimis = set([\n",
    "        p for ls in pd.concat(networks, ignore_index=True)[EXPERIMENT_TYPE]\n",
    "        for p in str(ls).split(',')\n",
    "        if str(p) not in NULL_VALUES\n",
    "    ])\n",
    "    for pmid in pmids:\n",
    "        if not session.query(Pubmed).filter_by(accession=pmid).count():\n",
    "            objects.append(Pubmed(accession=pmid))\n",
    "    for psimi in psimis:\n",
    "        if not session.query(Psimi).filter_by(accession=psimi).count():\n",
    "            objects.append(\n",
    "                Psimi(accession=psimi, description=mi_ont[psimi].name)\n",
    "            )\n",
    "    try:\n",
    "        session.add_all(objects)\n",
    "        session.commit()\n",
    "    except:\n",
    "        session.rollback()\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-15 12:02:04,119 scripts      INFO     Creating Interaction database entries.\n"
     ]
    }
   ],
   "source": [
    "    logger.info(\"Creating Interaction database entries.\")\n",
    "    interactions = {}\n",
    "    for interaction in session.query(Interaction).all():\n",
    "        a = p_manager.get_by_id(session, id=interaction.source)\n",
    "        b = p_manager.get_by_id(session, id=interaction.target)\n",
    "        interactions[(a.uniprot_id, b.uniprot_id)] = interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # Training should only update the is_training to true and leave other\n",
    "    # boolean fields alone.\n",
    "    logger.info(\"Creating training interaction entries.\")\n",
    "    generator = generate_interaction_tuples(training)\n",
    "    for i, (uniprot_a, uniprot_b, label, pmids, psimis) in enumerate(generator):\n",
    "        class_kwargs = feature_map[(uniprot_a, uniprot_b)]\n",
    "        class_kwargs[\"source\"] = protein_map[uniprot_a]\n",
    "        class_kwargs[\"target\"] = protein_map[uniprot_b]\n",
    "        class_kwargs[\"label\"] = label\n",
    "        class_kwargs[\"is_training\"] = True\n",
    "        class_kwargs[\"is_holdout\"] = False\n",
    "        class_kwargs[\"is_interactome\"] = False\n",
    "        entry = update_interaction(\n",
    "            session=session,\n",
    "            commit=False,\n",
    "            psimi_ls=psimi_string_to_list(session, psimi),\n",
    "            pmid_ls=pmid_string_to_list(session, pmids),\n",
    "            replace_fields=False,\n",
    "            override_boolean=False,\n",
    "            create_if_not_found=True,\n",
    "            match_taxon_id=9606,\n",
    "            verbose=False,\n",
    "            update_features=False,\n",
    "            **class_kwargs\n",
    "        )\n",
    "        interactions[(uniprot_a, uniprot_b,)] = entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # Testing should only update the is_holdout to true and leave other\n",
    "    # boolean fields alone.\n",
    "    logger.info(\"Creating holdout interaction entries.\")\n",
    "    generator = generate_interaction_tuples(testing)\n",
    "    for (uniprot_a, uniprot_b, label, pmids, psimis) in generator:\n",
    "        class_kwargs = feature_map[(uniprot_a, uniprot_b)]\n",
    "        class_kwargs[\"source\"] = protein_map[uniprot_a]\n",
    "        class_kwargs[\"target\"] = protein_map[uniprot_b]\n",
    "        class_kwargs[\"label\"] = label\n",
    "        class_kwargs[\"is_training\"] = False\n",
    "        class_kwargs[\"is_holdout\"] = True\n",
    "        class_kwargs[\"is_interactome\"] = False\n",
    "        entry = update_interaction(\n",
    "            session=session,\n",
    "            commit=False,\n",
    "            psimi_ls=psimi_string_to_list(session, psimi),\n",
    "            pmid_ls=pmid_string_to_list(session, pmids),\n",
    "            replace_fields=False,\n",
    "            override_boolean=False,\n",
    "            create_if_not_found=True,\n",
    "            match_taxon_id=9606,\n",
    "            verbose=False,\n",
    "            update_features=False,\n",
    "            **class_kwargs\n",
    "        )\n",
    "        interactions[(uniprot_a, uniprot_b,)] = entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "   # Common are in both kegg and hprd so should only update the is_training\n",
    "    # and is_holdout to true and leave other boolean fields alone.\n",
    "    logger.info(\"Creating training/holdout interaction entries.\")\n",
    "    generator = generate_interaction_tuples(common)\n",
    "    for (uniprot_a, uniprot_b, label, pmids, psimis) in generator:\n",
    "        class_kwargs = feature_map[(uniprot_a, uniprot_b)]\n",
    "        class_kwargs[\"source\"] = protein_map[uniprot_a]\n",
    "        class_kwargs[\"target\"] = protein_map[uniprot_b]\n",
    "        class_kwargs[\"label\"] = label\n",
    "        class_kwargs[\"is_training\"] = True\n",
    "        class_kwargs[\"is_holdout\"] = True\n",
    "        class_kwargs[\"is_interactome\"] = False\n",
    "        entry = update_interaction(\n",
    "            session=session,\n",
    "            commit=False,\n",
    "            psimi_ls=psimi_string_to_list(session, psimi),\n",
    "            pmid_ls=pmid_string_to_list(session, pmids),\n",
    "            replace_fields=False,\n",
    "            override_boolean=False,\n",
    "            create_if_not_found=True,\n",
    "            match_taxon_id=9606,\n",
    "            verbose=False,\n",
    "            update_features=False,\n",
    "            **class_kwargs\n",
    "        )\n",
    "        interactions[(uniprot_a, uniprot_b,)] = entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # Interactome should only update the is_interactome to true and leave other\n",
    "    # boolean fields alone.\n",
    "    logger.info(\"Creating interactome interaction entries.\")\n",
    "    generator = generate_interaction_tuples(interactome)\n",
    "    for (uniprot_a, uniprot_b, label, pmids, psimis) in generator:\n",
    "        class_kwargs = feature_map[(uniprot_a, uniprot_b)]\n",
    "        class_kwargs[\"source\"] = protein_map[uniprot_a]\n",
    "        class_kwargs[\"target\"] = protein_map[uniprot_b]\n",
    "        class_kwargs[\"label\"] = label\n",
    "        class_kwargs[\"is_training\"] = False\n",
    "        class_kwargs[\"is_holdout\"] = False\n",
    "        class_kwargs[\"is_interactome\"] = True\n",
    "        entry = update_interaction(\n",
    "            session=session,\n",
    "            commit=False,\n",
    "            psimi_ls=psimi_string_to_list(session, psimi),\n",
    "            pmid_ls=pmid_string_to_list(session, pmids),\n",
    "            replace_fields=False,\n",
    "            override_boolean=False,\n",
    "            create_if_not_found=True,\n",
    "            match_taxon_id=9606,\n",
    "            verbose=False,\n",
    "            update_features=False,\n",
    "            **class_kwargs\n",
    "        )\n",
    "        interactions[(uniprot_a, uniprot_b,)] = entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # Batch commit might be quicker than calling save on each interaction.\n",
    "    logger.info(\"Writing to database.\")\n",
    "    try:\n",
    "        session.commit()\n",
    "        session.close()\n",
    "    except:\n",
    "        session.rollback()\n",
    "        raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
