{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script runs classifier training over the entire training data and then\n",
    "output predictions over the interactome.\n",
    "\n",
    "Usage:\n",
    "  build_data.py [--clear_cache] [--init_database] [--verbose] [--n_jobs=J]\n",
    "  build_data.py -h | --help\n",
    "\n",
    "Options:\n",
    "  -h --help     Show this screen.\n",
    "  --verbose     Print intermediate output for debugging.\n",
    "  --n_jobs=J            Number of processes to run in parallel [default: 1]\n",
    "  --clear_cache      Delete previous bioservices KEGG/UniProt cache\n",
    "  --init_database    Save uniprot SwissProt/Trembl dumps into local database.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import logging\n",
    "from Bio import SwissProt\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from pyppi.base import delete_cache, delete_database\n",
    "from pyppi.base import parse_args, SOURCE, TARGET, LABEL\n",
    "\n",
    "from pyppi.data import bioplex_network_path, pina2_network_path\n",
    "from pyppi.data import bioplex_v4, pina2, innate_curated, innate_imported\n",
    "from pyppi.data import innate_i_network_path, innate_c_network_path\n",
    "from pyppi.data import interactome_network_path, full_training_network_path\n",
    "from pyppi.data import kegg_network_path, hprd_network_path\n",
    "from pyppi.data import save_uniprot_accession_map\n",
    "from pyppi.data import testing_network_path, training_network_path\n",
    "from pyppi.data import save_network_to_path\n",
    "from pyppi.data import save_ptm_labels\n",
    "from pyppi.data import default_db_path\n",
    "from pyppi.data import uniprot_sprot, uniprot_trembl\n",
    "\n",
    "from pyppi.database import begin_transaction\n",
    "from pyppi.database.models import Protein, Interaction\n",
    "from pyppi.database.managers import InteractionManager, ProteinManager\n",
    "\n",
    "from pyppi.data_mining.uniprot import parse_record_into_protein\n",
    "from pyppi.data_mining.uniprot import batch_map\n",
    "from pyppi.data_mining.generic import bioplex_func, mitab_func, pina_func\n",
    "from pyppi.data_mining.generic import generic_to_dataframe\n",
    "from pyppi.data_mining.hprd import hprd_to_dataframe\n",
    "from pyppi.data_mining.tools import process_interactions, make_interaction_frame\n",
    "from pyppi.data_mining.tools import remove_intersection, remove_labels\n",
    "from pyppi.data_mining.tools import map_network_accessions\n",
    "from pyppi.data_mining.kegg import download_pathway_ids, pathways_to_dataframe\n",
    "from pyppi.data_mining.ontology import get_active_instance\n",
    "from pyppi.data_mining.features import compute_interaction_features\n",
    "\n",
    "logger = logging.getLogger(\"scripts\")\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter(\n",
    "    '%(asctime)s %(name)-12s %(levelname)-8s %(message)s'\n",
    ")\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.propagate = False\n",
    "\n",
    "# -------------------------------------------------------------------------- #\n",
    "#                     MODIFY THESE TO SUIT YOUR NEEDS\n",
    "# -------------------------------------------------------------------------- #\n",
    "args = {\n",
    "    'n_jobs': 16,\n",
    "    'verbose': True,\n",
    "    'clear_cache': False,\n",
    "    'init_database': False\n",
    "}\n",
    "n_jobs = args['n_jobs']\n",
    "verbose = args['verbose']\n",
    "clear_cache = args['clear_cache']\n",
    "init_database = args['init_database']\n",
    "   \n",
    "i_manager = InteractionManager(verbose=verbose, match_taxon_id=9606)\n",
    "p_manager = ProteinManager(verbose=verbose, match_taxon_id=9606)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup the protein table in the database\n",
    "# -------------------------------------------------------------------------- #\n",
    "if clear_cache:\n",
    "    logger.info(\"Clearing Biopython/Bioservices cache.\")\n",
    "    delete_cache()\n",
    "    \n",
    "if init_database:\n",
    "    logger.info(\"Clearing database.\")\n",
    "    delete_database()\n",
    "    records = list(SwissProt.parse(uniprot_sprot())) + \\\n",
    "                list(SwissProt.parse(uniprot_trembl()))\n",
    "    proteins = [parse_record_into_protein(r) for r in records]\n",
    "    with begin_transaction() as session:\n",
    "        logger.info(\"Saving proteins to database.\")\n",
    "        for protein in proteins:\n",
    "            protein.save(session, commit=False)\n",
    "        try:\n",
    "            session.commit()\n",
    "        except:\n",
    "            session.rollback()\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-17 22:18:36,158 scripts      INFO     Building KEGG interactions.\n",
      "2017-12-17 22:18:59,466 pyppi        INFO     Warning: No reviewed acc found for hsa:387712.\n",
      "2017-12-17 22:18:59,467 pyppi        INFO     Warning: Could not map hsa:387712.\n",
      "2017-12-17 22:19:00,106 pyppi        INFO     Warning: No reviewed acc found for hsa:1056.\n",
      "2017-12-17 22:19:00,107 pyppi        INFO     Warning: Could not map hsa:1056.\n",
      "2017-12-17 22:19:01,610 pyppi        INFO     Warning: No reviewed acc found for hsa:729597.\n",
      "2017-12-17 22:19:01,611 pyppi        INFO     Warning: Could not map hsa:729597.\n",
      "2017-12-17 22:19:01,714 pyppi        INFO     Warning: More that one reviewed acc found for hsa:801: ['P0DP23', 'P0DP24', 'P0DP25']\n",
      "2017-12-17 22:19:01,847 pyppi        INFO     Warning: No reviewed acc found for hsa:442590.\n",
      "2017-12-17 22:19:01,848 pyppi        INFO     Warning: Could not map hsa:442590.\n",
      "2017-12-17 22:19:03,641 pyppi        INFO     Warning: More that one reviewed acc found for hsa:721: ['P0C0L4', 'P0C0L5']\n",
      "2017-12-17 22:19:04,418 pyppi        INFO     Warning: No reviewed acc found for hsa:100526760.\n",
      "2017-12-17 22:19:04,419 pyppi        INFO     Warning: Could not map hsa:100526760.\n",
      "2017-12-17 22:19:04,664 pyppi        INFO     Warning: More that one reviewed acc found for hsa:388372: ['P13236', 'Q8NHW4']\n",
      "2017-12-17 22:19:07,720 pyppi        INFO     Warning: No reviewed acc found for hsa:80319.\n",
      "2017-12-17 22:19:07,721 pyppi        INFO     Warning: Could not map hsa:80319.\n",
      "2017-12-17 22:19:09,912 pyppi        INFO     Warning: More that one reviewed acc found for hsa:875: ['P0DN79', 'P35520']\n",
      "2017-12-17 22:19:11,364 pyppi        INFO     Warning: No reviewed acc found for hsa:504191.\n",
      "2017-12-17 22:19:11,365 pyppi        INFO     Warning: Could not map hsa:504191.\n",
      "2017-12-17 22:19:14,781 pyppi        INFO     Warning: No reviewed acc found for hsa:115653.\n",
      "2017-12-17 22:19:14,782 pyppi        INFO     Warning: Could not map hsa:115653.\n",
      "2017-12-17 22:19:15,182 pyppi        INFO     Warning: No reviewed acc found for hsa:3108.\n",
      "2017-12-17 22:19:15,183 pyppi        INFO     Warning: Could not map hsa:3108.\n",
      "2017-12-17 22:19:17,449 pyppi        INFO     Warning: More that one reviewed acc found for hsa:3303: ['P0DMV8', 'P0DMV9']\n",
      "2017-12-17 22:19:19,254 pyppi        INFO     Warning: More that one reviewed acc found for hsa:808: ['P0DP23', 'P0DP24', 'P0DP25']\n",
      "2017-12-17 22:19:20,193 pyppi        INFO     Warning: More that one reviewed acc found for hsa:27113: ['Q96PG8', 'Q9BXH1']\n",
      "2017-12-17 22:19:23,661 pyppi        INFO     Warning: More that one reviewed acc found for hsa:805: ['P0DP23', 'P0DP24', 'P0DP25']\n",
      "2017-12-17 22:19:24,652 pyppi        INFO     Warning: No reviewed acc found for hsa:317749.\n",
      "2017-12-17 22:19:24,653 pyppi        INFO     Warning: Could not map hsa:317749.\n",
      "2017-12-17 22:19:24,802 pyppi        INFO     Warning: No reviewed acc found for hsa:100271927.\n",
      "2017-12-17 22:19:24,803 pyppi        INFO     Warning: Could not map hsa:100271927.\n",
      "2017-12-17 22:19:25,227 pyppi        INFO     Warning: More that one reviewed acc found for hsa:3304: ['P0DMV8', 'P0DMV9']\n",
      "2017-12-17 22:19:28,618 pyppi        INFO     Warning: No reviewed acc found for hsa:256892.\n",
      "2017-12-17 22:19:28,619 pyppi        INFO     Warning: Could not map hsa:256892.\n",
      "2017-12-17 22:19:30,106 pyppi        INFO     Warning: No reviewed acc found for hsa:100528021.\n",
      "2017-12-17 22:19:30,107 pyppi        INFO     Warning: Could not map hsa:100528021.\n",
      "2017-12-17 22:19:33,165 pyppi        INFO     Warning: No reviewed acc found for hsa:107987478.\n",
      "2017-12-17 22:19:33,166 pyppi        INFO     Warning: Could not map hsa:107987478.\n",
      "2017-12-17 22:19:34,625 pyppi        INFO     Warning: No reviewed acc found for hsa:3810.\n",
      "2017-12-17 22:19:34,626 pyppi        INFO     Warning: Could not map hsa:3810.\n",
      "2017-12-17 22:19:34,646 pyppi        INFO     Warning: No reviewed acc found for hsa:441024.\n",
      "2017-12-17 22:19:34,646 pyppi        INFO     Warning: Could not map hsa:441024.\n",
      "2017-12-17 22:19:35,428 pyppi        INFO     Warning: No reviewed acc found for hsa:26686.\n",
      "2017-12-17 22:19:35,429 pyppi        INFO     Warning: Could not map hsa:26686.\n",
      "2017-12-17 22:19:36,952 pyppi        INFO     Warning: More that one reviewed acc found for hsa:1442: ['P0DML2', 'P0DML3']\n",
      "2017-12-17 22:19:37,182 pyppi        INFO     Warning: More that one reviewed acc found for hsa:102724560: ['P0DN79', 'P35520']\n",
      "2017-12-17 22:19:38,741 pyppi        INFO     Warning: No reviewed acc found for hsa:107181291.\n",
      "2017-12-17 22:19:38,742 pyppi        INFO     Warning: Could not map hsa:107181291.\n",
      "2017-12-17 22:19:41,253 pyppi        INFO     Warning: No reviewed acc found for hsa:26687.\n",
      "2017-12-17 22:19:41,254 pyppi        INFO     Warning: Could not map hsa:26687.\n",
      "2017-12-17 22:19:43,456 pyppi        INFO     Warning: More that one reviewed acc found for hsa:2778: ['O95467', 'P63092']\n"
     ]
    }
   ],
   "source": [
    "# Construct all the networks\n",
    "# -------------------------------------------------------------------------- #\n",
    "logger.info(\"Building KEGG interactions.\")\n",
    "with begin_transaction() as session:\n",
    "    pathways = download_pathway_ids('hsa')\n",
    "    kegg = pathways_to_dataframe(\n",
    "        session=session,\n",
    "        pathway_ids=pathways,\n",
    "        map_to_uniprot=True,\n",
    "        drop_nan=True,\n",
    "        allow_self_edges=True,\n",
    "        allow_duplicates=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-17 22:19:45,380 scripts      INFO     Building HPRD interactions.\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Building HPRD interactions.\")\n",
    "hprd = hprd_to_dataframe(\n",
    "    drop_nan=True,\n",
    "    allow_self_edges=True,\n",
    "    allow_duplicates=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-17 22:19:48,282 scripts      INFO     Building Interactome interactions.\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Building Interactome interactions.\")\n",
    "bioplex = generic_to_dataframe(\n",
    "    f_input=bioplex_v4(),\n",
    "    parsing_func=bioplex_func,\n",
    "    drop_nan=True,\n",
    "    allow_self_edges=True,\n",
    "    allow_duplicates=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pina2 = generic_to_dataframe(\n",
    "    f_input=pina2(),\n",
    "    parsing_func=pina_func,\n",
    "    drop_nan=True,\n",
    "    allow_self_edges=True,\n",
    "    allow_duplicates=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "innate_c = generic_to_dataframe(\n",
    "    f_input=innate_curated(),\n",
    "    parsing_func=mitab_func,\n",
    "    drop_nan=True,\n",
    "    allow_self_edges=True,\n",
    "    allow_duplicates=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "innate_i = generic_to_dataframe(\n",
    "    f_input=innate_imported(),\n",
    "    parsing_func=mitab_func,\n",
    "    drop_nan=True,\n",
    "    allow_self_edges=True,\n",
    "    allow_duplicates=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-17 22:20:15,466 scripts      INFO     Mapping to most recent uniprot accessions.\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Mapping to most recent uniprot accessions.\")\n",
    "# Get a set of all the unique uniprot accessions\n",
    "networks = [kegg, hprd, bioplex, pina2, innate_i, innate_c]\n",
    "sources = set(p for df in networks for p in df.source.values)\n",
    "targets = set(p for df in networks for p in df.target.values)\n",
    "accessions = list(sources | targets)\n",
    "with begin_transaction() as session:\n",
    "    accession_mapping = batch_map(\n",
    "        session=session,\n",
    "        allow_download=False,\n",
    "        accessions=accessions,\n",
    "        keep_unreviewed=True,\n",
    "        match_taxon_id=9606\n",
    "    )\n",
    "save_uniprot_accession_map(accession_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-17 22:28:36,112 scripts      INFO     Mapping each network to the most recent uniprot accessions.\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Mapping each network to the most recent uniprot accessions.\")\n",
    "kegg = map_network_accessions(\n",
    "    interactions=kegg, accession_map=accession_mapping,\n",
    "    drop_nan=True, allow_self_edges=True,\n",
    "    allow_duplicates=False, min_counts=None, merge=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hprd = map_network_accessions(\n",
    "    interactions=hprd, accession_map=accession_mapping,\n",
    "    drop_nan=True, allow_self_edges=True,\n",
    "    allow_duplicates=False, min_counts=None, merge=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pina2 = map_network_accessions(\n",
    "    interactions=pina2, accession_map=accession_mapping,\n",
    "    drop_nan=True, allow_self_edges=True,\n",
    "    allow_duplicates=False, min_counts=None, merge=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bioplex = map_network_accessions(\n",
    "    interactions=bioplex, accession_map=accession_mapping,\n",
    "    drop_nan=True, allow_self_edges=True,\n",
    "    allow_duplicates=False, min_counts=None, merge=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "innate_c = map_network_accessions(\n",
    "    interactions=innate_c, accession_map=accession_mapping,\n",
    "    drop_nan=True, allow_self_edges=True,\n",
    "    allow_duplicates=False, min_counts=None, merge=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "innate_i = map_network_accessions(\n",
    "    interactions=innate_i, accession_map=accession_mapping,\n",
    "    drop_nan=True, allow_self_edges=True,\n",
    "    allow_duplicates=False, min_counts=None, merge=False\n",
    ")\n",
    "networks = [hprd, kegg, bioplex, pina2, innate_i, innate_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-17 22:28:47,493 scripts      INFO     Saving raw networks.\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Saving raw networks.\")\n",
    "save_network_to_path(kegg, kegg_network_path)\n",
    "save_network_to_path(hprd, hprd_network_path)\n",
    "save_network_to_path(pina2, pina2_network_path)\n",
    "save_network_to_path(bioplex, bioplex_network_path)\n",
    "save_network_to_path(innate_i, innate_i_network_path)\n",
    "save_network_to_path(innate_c, innate_c_network_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-17 22:28:48,890 scripts      INFO     Building and saving processed networks.\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Building and saving processed networks.\")\n",
    "hprd_test_labels = ['dephosphorylation', 'phosphorylation']\n",
    "hprd_train_labels = set([l for l in hprd[LABEL] if l not in hprd_test_labels])\n",
    "train_hprd = remove_labels(hprd, hprd_test_labels)\n",
    "training = pd.concat([kegg, train_hprd], ignore_index=True)\n",
    "testing = remove_intersection(remove_labels(hprd, hprd_train_labels), kegg)\n",
    "\n",
    "# Some ppis will be the same between training/testing sets but\n",
    "# with different labels. Put all the ppis appearing in testing\n",
    "# with a different label compared to the same instance in training\n",
    "# into the training set. This way we can keep the testing and\n",
    "# training sets completely disjoint.\n",
    "labels = []\n",
    "sources= []\n",
    "targets = []\n",
    "for (a, b, l) in zip(testing[SOURCE], testing[TARGET], testing[LABEL]):\n",
    "    if (a, b) in zip(training[SOURCE], training[TARGET]):\n",
    "        sources.append(a)\n",
    "        targets.append(b)\n",
    "        labels.append(l)\n",
    "common_ppis = make_interaction_frame(sources, targets, labels)\n",
    "training = pd.concat([training, common_ppis], ignore_index=True)\n",
    "testing = remove_intersection(testing, training)\n",
    "full_training = pd.concat([training, testing], ignore_index=True)\n",
    "\n",
    "testing = process_interactions(\n",
    "    interactions=testing, drop_nan=True,\n",
    "    allow_duplicates=False, allow_self_edges=True,\n",
    "    exclude_labels=None, min_counts=5, merge=True\n",
    ")\n",
    "training = process_interactions(\n",
    "    interactions=training,\n",
    "    drop_nan=True, allow_duplicates=False, allow_self_edges=True,\n",
    "    exclude_labels=None, min_counts=5, merge=True\n",
    ")\n",
    "full_training = process_interactions(\n",
    "    interactions=full_training,\n",
    "    drop_nan=True, allow_duplicates=False, allow_self_edges=True,\n",
    "    exclude_labels=None, min_counts=5, merge=True\n",
    ")\n",
    "\n",
    "labels = list(training[LABEL]) + list(testing[LABEL])\n",
    "ptm_labels = set(l for merged in labels for l in merged.split(','))\n",
    "save_ptm_labels(ptm_labels)\n",
    "\n",
    "interactome_networks = [bioplex, pina2, innate_i, innate_c]\n",
    "interactome = pd.concat(interactome_networks, ignore_index=True)\n",
    "interactome = process_interactions(\n",
    "    interactions=interactome, drop_nan=True,\n",
    "    allow_duplicates=False, allow_self_edges=True,\n",
    "    exclude_labels=None, min_counts=None, merge=True\n",
    ")\n",
    "save_network_to_path(interactome, interactome_network_path)\n",
    "save_network_to_path(training, training_network_path)\n",
    "save_network_to_path(testing, testing_network_path)\n",
    "save_network_to_path(full_training, full_training_network_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-17 22:29:06,632 scripts      INFO     Saving Interaction records to database.\n",
      "2017-12-17 22:29:12,135 scripts      INFO     Computing features.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=16)]: Done 177 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=16)]: Done 717 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=16)]: Done 2600 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=16)]: Done 4214 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=16)]: Done 6414 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=16)]: Done 9014 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=16)]: Done 12014 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=16)]: Done 18815 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=16)]: Done 25252 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=16)]: Done 28402 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=16)]: Done 40294 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=16)]: Done 51148 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=16)]: Done 59248 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=16)]: Done 69892 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=16)]: Done 85801 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=16)]: Done 90751 tasks      | elapsed:  6.3min\n",
      "[Parallel(n_jobs=16)]: Done 98524 tasks      | elapsed:  7.0min\n",
      "[Parallel(n_jobs=16)]: Done 107184 tasks      | elapsed:  7.7min\n",
      "[Parallel(n_jobs=16)]: Done 116838 tasks      | elapsed:  8.6min\n",
      "[Parallel(n_jobs=16)]: Done 123033 tasks      | elapsed:  9.4min\n",
      "[Parallel(n_jobs=16)]: Done 127601 tasks      | elapsed: 10.2min\n",
      "[Parallel(n_jobs=16)]: Done 136601 tasks      | elapsed: 11.2min\n",
      "[Parallel(n_jobs=16)]: Done 146001 tasks      | elapsed: 12.9min\n",
      "[Parallel(n_jobs=16)]: Done 154905 tasks      | elapsed: 13.7min\n",
      "[Parallel(n_jobs=16)]: Done 165105 tasks      | elapsed: 14.6min\n",
      "[Parallel(n_jobs=16)]: Done 176403 tasks      | elapsed: 15.7min\n",
      "[Parallel(n_jobs=16)]: Done 188075 tasks      | elapsed: 16.8min\n",
      "[Parallel(n_jobs=16)]: Done 199475 tasks      | elapsed: 17.6min\n",
      "[Parallel(n_jobs=16)]: Done 210677 tasks      | elapsed: 18.6min\n",
      "[Parallel(n_jobs=16)]: Done 222877 tasks      | elapsed: 19.4min\n",
      "[Parallel(n_jobs=16)]: Done 239965 tasks      | elapsed: 20.3min\n",
      "[Parallel(n_jobs=16)]: Done 252965 tasks      | elapsed: 21.3min\n",
      "[Parallel(n_jobs=16)]: Done 266121 tasks      | elapsed: 22.7min\n",
      "[Parallel(n_jobs=16)]: Done 279201 out of 279201 | elapsed: 24.0min finished\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__': # guard here to protect Joblib Parallel call.\n",
    "    logger.info(\"Saving Interaction records to database.\")\n",
    "    with begin_transaction() as session:\n",
    "        protein_map = p_manager.uniprotid_entry_map(session)\n",
    "        ppis = [\n",
    "            (protein_map[a], protein_map[b])\n",
    "            for network in [training, testing, interactome]\n",
    "            for (a, b) in zip(network[SOURCE], network[TARGET])\n",
    "        ]\n",
    "\n",
    "    feature_map = {}\n",
    "    logger.info(\"Computing features.\")\n",
    "    features = Parallel(n_jobs=n_jobs, backend=\"multiprocessing\", verbose=verbose)(\n",
    "        delayed(compute_interaction_features)(source, target)\n",
    "        for (source, target) in ppis\n",
    "    )\n",
    "    for (source, target), features in zip(ppis, features):    \n",
    "        feature_map[(source.uniprot_id, target.uniprot_id)] = features\n",
    "\n",
    "    # Training\n",
    "    interactions = {}\n",
    "    for (uniprot_a, uniprot_b, label) in zip(training[SOURCE], training[TARGET], training[LABEL]):\n",
    "        entry = interactions.get((uniprot_a, uniprot_b), None)\n",
    "        if entry is None:\n",
    "            interaction = Interaction(\n",
    "                source=protein_map[uniprot_a].id,\n",
    "                target=protein_map[uniprot_b].id,\n",
    "                is_training=True,\n",
    "                is_holdout=False,\n",
    "                is_interactome=False,\n",
    "                label=label,\n",
    "                **feature_map[(uniprot_a, uniprot_b)]\n",
    "            )\n",
    "            interactions[(uniprot_a, uniprot_b)] = interaction\n",
    "        else:\n",
    "            # If this raises, then the training/testing are not disjoint as expected.\n",
    "            raise ValueError(\"Interaction already exists.\")\n",
    "\n",
    "    # Testing/Holdout\n",
    "    for (uniprot_a, uniprot_b, label) in zip(testing[SOURCE], testing[TARGET], testing.label):\n",
    "        entry = interactions.get((uniprot_a, uniprot_b), None)\n",
    "        if entry is None:\n",
    "            interaction = Interaction(\n",
    "                source=protein_map[uniprot_a].id,\n",
    "                target=protein_map[uniprot_b].id,\n",
    "                is_training=False,\n",
    "                is_holdout=True,\n",
    "                is_interactome=False,\n",
    "                label=label,\n",
    "                **feature_map[(uniprot_a, uniprot_b)]\n",
    "            )\n",
    "            interactions[(uniprot_a, uniprot_b)] = interaction\n",
    "        else:\n",
    "            # If this raises, then the training/testing are not disjoint as expected.\n",
    "            raise ValueError(\"Interaction already exists.\")\n",
    "            \n",
    "    # Interactome\n",
    "    for (uniprot_a, uniprot_b) in zip(interactome[SOURCE], interactome[TARGET]):\n",
    "        entry = interactions.get((uniprot_a, uniprot_b), None)\n",
    "        if entry is None:\n",
    "            interaction = Interaction(\n",
    "                source=protein_map[uniprot_a].id,\n",
    "                target=protein_map[uniprot_b].id,\n",
    "                is_training=False,\n",
    "                is_holdout=False,\n",
    "                is_interactome=True,\n",
    "                label=None,\n",
    "                **feature_map[(uniprot_a, uniprot_b)]\n",
    "            )\n",
    "            interactions[(uniprot_a, uniprot_b)] = interaction\n",
    "        else:\n",
    "            entry.is_interactome = True\n",
    "            interactions[(uniprot_a, uniprot_b)] = entry\n",
    "\n",
    "    with begin_transaction() as session:\n",
    "        logger.info(\"Writing to database.\")\n",
    "        for interaction in interactions.values():\n",
    "            interaction.save(session, commit=False)\n",
    "        try:\n",
    "            session.commit()\n",
    "        except:\n",
    "            session.rollback()\n",
    "            raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
