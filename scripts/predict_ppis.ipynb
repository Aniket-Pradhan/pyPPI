{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "\"\"\"\n",
    "This script runs classifier training over the entire training data and then\n",
    "output predictions over the interactome.\n",
    "\n",
    "Usage:\n",
    "  predict_ppis.py [--interpro] [--pfam] [--mf] [--cc] [--bp]\n",
    "                  [--use_cache] [--retrain] [--induce] [--verbose]\n",
    "                  [--model=M] [--n_jobs=J] [--n_splits=S] [--n_iterations=I]\n",
    "                  [--input=FILE] [--output=FILE] [--directory=DIR]\n",
    "  predict_ppis.py -h | --help\n",
    "\n",
    "Options:\n",
    "  -h --help     Show this screen.\n",
    "  --interpro    Use interpro domains in features.\n",
    "  --pfam        Use Pfam domains in features.\n",
    "  --mf          Use Molecular Function Gene Ontology in features.\n",
    "  --cc          Use Cellular Compartment Gene Ontology in features.\n",
    "  --bp          Use Biological Process Gene Ontology in features.\n",
    "  --induce      Use ULCA inducer over Gene Ontology.\n",
    "  --verbose     Print intermediate output for debugging.\n",
    "  --binary      Use binary feature encoding instead of ternary.\n",
    "  --use_cache   Use cached features if available.\n",
    "  --retrain     Re-train classifier instead of loading previous version. If\n",
    "                using a previous version, you must use the same selection of\n",
    "                features along with the same induce setting.\n",
    "  --model=M         A binary classifier from Scikit-Learn implementing fit,\n",
    "                    predict and predict_proba [default: LogisticRegression].\n",
    "                    Ignored if using 'retrain'.\n",
    "  --n_jobs=J        Number of processes to run in parallel [default: 1]\n",
    "  --n_splits=S      Number of cross-validation splits used during randomized\n",
    "                    grid search [default: 5]\n",
    "  --n_iterations=I  Number of randomized grid search iterations [default: 60]\n",
    "  --input=FILE      Uniprot edge-list, with a path directory that absolute or\n",
    "                    relative to this script. Entries must be tab separated with\n",
    "                    header columns 'source' and 'target'. [default: None]\n",
    "  --output=FILE     Output file name [default: predictions.tsv]\n",
    "  --directory=DIR   Absolute or relative output directory [default: ./results/]\n",
    "\"\"\"\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from datetime import datetime\n",
    "\n",
    "from pyppi.base import parse_args, su_make_dir, chunk_list\n",
    "from pyppi.base import P1, P2, G1, G2, SOURCE, TARGET\n",
    "from pyppi.data import load_network_from_path, load_ptm_labels\n",
    "from pyppi.data import full_training_network_path, generic_io\n",
    "from pyppi.data import interactome_network_path, classifier_path\n",
    "\n",
    "from pyppi.models import make_classifier, get_parameter_distribution_for_model\n",
    "\n",
    "from pyppi.database import begin_transaction\n",
    "from pyppi.database.models import Interaction\n",
    "from pyppi.database.managers import InteractionManager, ProteinManager\n",
    "from pyppi.database.managers import format_interactions_for_sklearn\n",
    "\n",
    "from pyppi.data_mining.tools import xy_from_interaction_frame\n",
    "from pyppi.data_mining.generic import edgelist_func, generic_to_dataframe\n",
    "from pyppi.data_mining.tools import map_network_accessions\n",
    "from pyppi.data_mining.uniprot import batch_map\n",
    "from pyppi.data_mining.features import compute_interaction_features\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"scripts\")\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter(\n",
    "    '%(asctime)s %(name)-12s %(levelname)-8s %(message)s'\n",
    ")\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.propagate = False\n",
    "\n",
    "args = parse_args({\n",
    "    '--n_jobs': 14,\n",
    "    '--n_splits': 5,\n",
    "    '--n_iterations': 120,\n",
    "    '--verbose': True,\n",
    "    \n",
    "    '--mf': True,\n",
    "    '--bp': True,\n",
    "    '--cc': True,\n",
    "    '--interpro':True,\n",
    "    '--pfam': True,\n",
    "    '--induce': True,\n",
    "    '--binary': False,\n",
    "    \n",
    "    '--model': 'LogisticRegression',\n",
    "    '--use_cache': True,\n",
    "    '--output': 'predictions.tsv',\n",
    "    '--input': './data/test.tsv',\n",
    "    '--directory': './results/',\n",
    "    '--retrain': True\n",
    "})\n",
    "\n",
    "n_jobs = args['n_jobs']\n",
    "n_splits = args['n_splits']\n",
    "rcv_iter = args['n_iterations']\n",
    "induce = args['induce']\n",
    "verbose = args['verbose']\n",
    "selection = args['selection']\n",
    "model = args['model']\n",
    "use_feature_cache = args['use_cache']\n",
    "out_file = args['output']\n",
    "input_file = args['input']\n",
    "direc = args['directory']\n",
    "retrain = args['retrain']\n",
    "use_binary = args['binary']\n",
    "\n",
    "\n",
    "# Set up the folder for each experiment run named after the current time\n",
    "# -------------------------------------------------------------------- #\n",
    "folder = datetime.now().strftime(\"pred_%y-%m-%d_%H-%M-%S\")\n",
    "direc = \"{}/{}/\".format(direc, folder)\n",
    "su_make_dir(direc)\n",
    "json.dump(args, fp=open(\"{}/settings.json\".format(direc), 'w'), indent=4, sort_keys=True)\n",
    "i_manager = InteractionManager(verbose=verbose, match_taxon_id=9606)\n",
    "p_manager = ProteinManager(verbose=verbose, match_taxon_id=9606)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-16 12:33:32,772 scripts      INFO     Loading custom ppi data...\n",
      "2017-12-16 12:33:52,150 scripts      INFO     Computing features.\n",
      "[Parallel(n_jobs=14)]: Done   0 out of   0 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "# Get the input edge-list ready\n",
    "# -------------------------------------------------------------------- #\n",
    "if __name__ == \"__main__\":\n",
    "    with begin_transaction() as session:\n",
    "        labels = i_manager.training_labels(session, include_holdout=True)\n",
    "        training = i_manager.training_interactions(session, filter_out_holdout=False)\n",
    "        \n",
    "        if input_file == 'default':\n",
    "            logger.info(\"Loading interactome data...\")\n",
    "            testing = i_manager.interactome_interactions(\n",
    "                session=session,\n",
    "                filter_out_holdout=False,\n",
    "                filter_out_training=False\n",
    "            )\n",
    "        else:\n",
    "            logger.info(\"Loading custom ppi data...\")\n",
    "            testing = generic_to_dataframe(\n",
    "                f_input=generic_io(input_file),\n",
    "                parsing_func=edgelist_func,\n",
    "                drop_nan=True,\n",
    "                allow_self_edges=True,\n",
    "                allow_duplicates=True\n",
    "            )\n",
    "            sources = set(p for p in testing.source.values)\n",
    "            targets = set(p for p in testing.target.values)\n",
    "            accessions = list(sources | targets)\n",
    "            accession_mapping = batch_map(\n",
    "                session=session,\n",
    "                accessions=accessions,\n",
    "                keep_unreviewed=True,\n",
    "                match_taxon_id=9606,\n",
    "                allow_download=True\n",
    "            )\n",
    "            testing_network = map_network_accessions(\n",
    "                interactions=testing, accession_map=accession_mapping,\n",
    "                drop_nan=True, allow_self_edges=True,\n",
    "                allow_duplicates=False, min_counts=None, merge=False\n",
    "            )\n",
    "\n",
    "            # Compute features for new ppis\n",
    "            testing = []\n",
    "            feature_map = {}\n",
    "            protein_map = p_manager.uniprotid_entry_map(session)\n",
    "            ppis = [\n",
    "                (protein_map[a], protein_map[b])\n",
    "                for (a, b) in zip(testing_network[SOURCE], testing_network[TARGET])\n",
    "                if i_manager.get_by_source_target(session, a, b) is None\n",
    "            ]\n",
    "\n",
    "            logger.info(\"Computing features.\")\n",
    "            features = Parallel(n_jobs=n_jobs, backend=\"multiprocessing\", verbose=verbose)(\n",
    "                delayed(compute_interaction_features)(source, target)\n",
    "                for (source, target) in ppis\n",
    "            )\n",
    "            for (source, target), features in zip(ppis, features):    \n",
    "                feature_map[(source.uniprot_id, target.uniprot_id)] = features\n",
    "\n",
    "            for (a, b) in zip(testing_network[SOURCE], testing_network[TARGET]):\n",
    "                entry = i_manager.get_by_source_target(session, a, b)\n",
    "                if entry is None:\n",
    "                    logger.info(\"Creating new Interaction ({},{}).\".format(a, b))\n",
    "                    entry = Interaction(\n",
    "                        source=protein_map[a], target=protein_map[b],\n",
    "                        is_interactome=False,\n",
    "                        is_training=False,\n",
    "                        is_holdout=False,\n",
    "                        label=None,\n",
    "                        **feature_map[(a, b)]\n",
    "                    )\n",
    "                    entry.save(session, commit=True)\n",
    "                testing.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-16 12:34:03,501 scripts      INFO     Preparing training and testing data...\n"
     ]
    }
   ],
   "source": [
    "# Get the features into X, and multilabel y indicator format\n",
    "# -------------------------------------------------------------------- #\n",
    "logger.info(\"Preparing training and testing data...\")\n",
    "X_train, y_train = format_interactions_for_sklearn(training, selection)\n",
    "X_test, _ = format_interactions_for_sklearn(testing, selection)\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=sorted(labels))\n",
    "mlb.fit(y_train)\n",
    "y_train = mlb.transform(y_train)\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True if use_binary else False)\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-16 12:40:52,316 scripts      INFO     Making classifier...\n",
      "2017-12-16 12:40:52,316 scripts      INFO     Making classifier...\n"
     ]
    }
   ],
   "source": [
    "# Make the estimators and BR classifier\n",
    "# -------------------------------------------------------------------- #\n",
    "if retrain or not os.path.isfile(classifier_path):\n",
    "    logger.info(\"Making classifier...\")\n",
    "    params = get_parameter_distribution_for_model(model)\n",
    "    random_cv = RandomizedSearchCV(\n",
    "        cv=n_splits,\n",
    "        n_iter=rcv_iter,\n",
    "        n_jobs=n_jobs,\n",
    "        refit=True, \n",
    "        random_state=42,\n",
    "        scoring='f1', \n",
    "        error_score=0,\n",
    "        param_distributions=params,\n",
    "        estimator=make_classifier(model)\n",
    "    )\n",
    "    clf = OneVsRestClassifier(estimator=random_cv, n_jobs=1)\n",
    "\n",
    "    # Fit the complete training data and make predictions.\n",
    "    logging.info(\"Fitting data...\")\n",
    "    clf.fit(X_train, y_train)\n",
    "    joblib.dump(clf, classifier_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads a previously (or recently trained) classifier from disk\n",
    "# and then performs the predictions on the new dataset.\n",
    "# -------------------------------------------------------------------- #\n",
    "logging.info(\"Making predictions...\")\n",
    "clf = joblib.load(classifier_path)\n",
    "predictions = clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the predictions to a tsv file\n",
    "# -------------------------------------------------------------------- #\n",
    "logging.info(\"Writing results to file...\")\n",
    "data_dict = {\n",
    "    P1: [s for (s, _) in X_test_ppis],\n",
    "    P2: [t for (_, t) in X_test_ppis],\n",
    "    \"sum\": np.sum(predictions, axis=1)\n",
    "}\n",
    "\n",
    "for idx, label in enumerate(mlb.classes):\n",
    "    data_dict[label] = predictions[:, idx]\n",
    "\n",
    "columns = [P1, P2, G1, G2] + list(sorted(mlb.classes)) + ['sum']\n",
    "df = pd.DataFrame(data=data_dict, columns=columns)\n",
    "\n",
    "accession_gene_map = {p.uniprot_id: p.gene_id for p in protein_map.values()}\n",
    "df['{}'.format(G1)] = df.apply(\n",
    "    func=lambda row: accession_gene_map.get(row[P1], ['-'])[0] or '-',\n",
    "    axis=1)\n",
    "df['{}'.format(G2)] = df.apply(\n",
    "    func=lambda row: accession_gene_map.get(row[P2], ['-'])[0] or '-', \n",
    "    axis=1)\n",
    "df.to_csv(\"{}/{}\".format(direc, out_file), sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the proportion of the interactome classified at a threshold value, t.\n",
    "logging.info(\"Writing results to file...\")\n",
    "thresholds = np.arange(0.0, 1.05, 0.05)\n",
    "proportions = np.zeros_like(thresholds)\n",
    "for i, t in enumerate(thresholds):\n",
    "    classified = sum(map(lambda p: np.max(p) >= t, predictions))\n",
    "    proportion = classified / predictions.shape[0]\n",
    "    proportions[i] = proportion\n",
    "\n",
    "with open(\"{}/thresholds.csv\".format(direc), 'wt') as fp:\n",
    "    for (t, p) in zip(thresholds, proportions):\n",
    "        fp.write(\"{},{}\\n\".format(t, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
