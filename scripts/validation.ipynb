{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script runs the bootstrap kfold validation experiments as used in\n",
    "the publication.\n",
    "\n",
    "Usage:\n",
    "  validation.py [--interpro] [--pfam] [--mf] [--cc] [--bp]\n",
    "             [--use_cache] [--induce] [--verbose] [--abs] [--top=T]\n",
    "             [--model=M] [--n_jobs=J] [--n_splits=S] [--n_iterations=I]\n",
    "             [--h_iterations=H] [--directory=DIR]\n",
    "  validation.py -h | --help\n",
    "\n",
    "Options:\n",
    "  -h --help     Show this screen.\n",
    "  --interpro    Use interpro domains in features.\n",
    "  --pfam        Use Pfam domains in features.\n",
    "  --mf          Use Molecular Function Gene Ontology in features.\n",
    "  --cc          Use Cellular Compartment Gene Ontology in features.\n",
    "  --bp          Use Biological Process Gene Ontology in features.\n",
    "  --binary      Use binary feature encoding instead of ternary.\n",
    "  --induce      Use ULCA inducer over Gene Ontology.\n",
    "  --verbose     Print intermediate output for debugging.\n",
    "  --abs         Take the absolute value of feature weights when computing top\n",
    "                features.\n",
    "  --top=T       Top T features for each label to log [default: 25]\n",
    "  --model=M         A binary classifier from Scikit-Learn implementing fit,\n",
    "                    predict and predict_proba [default: LogisticRegression]\n",
    "  --n_jobs=J        Number of processes to run in parallel [default: 1]\n",
    "  --n_splits=S      Number of cross-validation splits [default: 5]\n",
    "  --h_iterations=H  Number of hyperparameter tuning\n",
    "                    iterations per fold [default: 60]\n",
    "  --n_iterations=I  Number of bootstrap iterations [default: 5]\n",
    "  --directory=DIR   Output directory [default: ./results/]\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from itertools import product\n",
    "from operator import itemgetter\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from docopt import docopt\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "from pyppi.base import parse_args, su_make_dir\n",
    "from pyppi.data import load_network_from_path, load_ptm_labels\n",
    "from pyppi.data import testing_network_path, training_network_path\n",
    "from pyppi.data import get_term_description, ipr_name_map, pfam_name_map\n",
    "\n",
    "from pyppi.models.utilities import get_coefs, top_n_features\n",
    "from pyppi.models import make_classifier, get_parameter_distribution_for_model\n",
    "from pyppi.models import supported_estimators\n",
    "from pyppi.model_selection.scoring import fdr_score, specificity\n",
    "from pyppi.model_selection.sampling import IterativeStratifiedKFold\n",
    "\n",
    "from pyppi.data_mining.ontology import get_active_instance\n",
    "\n",
    "from pyppi.database import begin_transaction\n",
    "from pyppi.database.models import Interaction\n",
    "from pyppi.database.managers import InteractionManager, format_interactions_for_sklearn\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import (\n",
    "    recall_score, make_scorer,\n",
    "    label_ranking_average_precision_score,\n",
    "    label_ranking_loss,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(\"scripts\")\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter(\n",
    "    '%(asctime)s %(name)-12s %(levelname)-8s %(message)s'\n",
    ")\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.propagate = False\n",
    "\n",
    "args = parse_args({\n",
    "    '--n_jobs': 16,\n",
    "    '--n_splits': 5,\n",
    "    '--n_iterations': 5,\n",
    "    '--h_iterations': 60,\n",
    "    \n",
    "    '--mf': True,\n",
    "    '--bp': True,\n",
    "    '--cc': True,\n",
    "    '--interpro': True,\n",
    "    '--pfam': True,\n",
    "    '--induce': True,\n",
    "    '--binary': False,    \n",
    "    '--abs': False,\n",
    "    '--top': 25,\n",
    "    \n",
    "    '--verbose': True,\n",
    "    '--model': 'LogisticRegression',\n",
    "    '--directory': './results/'\n",
    "})\n",
    "n_jobs = args['n_jobs']\n",
    "n_splits = args['n_splits']\n",
    "n_iter = args['n_iterations']\n",
    "induce = args['induce']\n",
    "verbose = args['verbose']\n",
    "selection = args['selection']\n",
    "model = args['model']\n",
    "use_feature_cache = args['use_cache']\n",
    "direc = args['directory']\n",
    "hyperparam_iter = args['h_iterations']\n",
    "get_top_n = args['top']\n",
    "abs_weights = args['abs']\n",
    "use_binary = args['binary']\n",
    "\n",
    "# Set up the folder for each experiment run named after the current time\n",
    "folder = datetime.now().strftime(\"val_%y-%m-%d_%H-%M\")\n",
    "direc = \"{}/{}/\".format(direc, folder)\n",
    "su_make_dir(direc)\n",
    "json.dump(\n",
    "    args, \n",
    "    fp=open(\"{}/settings.json\".format(direc), 'w'),\n",
    "    indent=4, \n",
    "    sort_keys=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-19 12:03:31,047 scripts      INFO     Loading training and testing data.\n"
     ]
    }
   ],
   "source": [
    "# Load all the training data, features etc.\n",
    "# ------------------------------------------------------------------- #\n",
    "logger.info(\"Loading training and testing data.\")\n",
    "ipr_map = ipr_name_map(short_names=False)\n",
    "pfam_map = pfam_name_map()\n",
    "go_dag = get_active_instance()\n",
    "i_manager = InteractionManager(verbose=verbose, match_taxon_id=9606)\n",
    "\n",
    "with begin_transaction() as session:\n",
    "    labels = i_manager.training_labels(session, include_holdout=False)\n",
    "    training = i_manager.training_interactions(session, filter_out_holdout=True)\n",
    "    testing = i_manager.holdout_interactions(session, filter_out_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-19 12:03:40,730 scripts      INFO     Preparing training and testing data.\n"
     ]
    }
   ],
   "source": [
    "# Get the features into X, and multilabel y indicator format\n",
    "# -------------------------------------------------------------------- #\n",
    "logger.info(\"Preparing training and testing data.\")\n",
    "X_train, y_train = format_interactions_for_sklearn(training, selection)\n",
    "X_test, y_test = format_interactions_for_sklearn(testing, selection)\n",
    "\n",
    "logging.info(\"Computing class distributions.\")\n",
    "json.dump(\n",
    "    Counter([l for ls in y_train for l in ls]),\n",
    "    fp=open(\"{}/training_distribution.json\".format(direc), 'w'),\n",
    "    indent=4, sort_keys=True\n",
    ")\n",
    "json.dump(\n",
    "    Counter([l for ls in y_test for l in ls]),\n",
    "    fp=open(\"{}/testing_distribution.json\".format(direc), 'w'),\n",
    "    indent=4, sort_keys=True\n",
    ")\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=labels, sparse_output=False)\n",
    "y_train = mlb.fit_transform(y_train)\n",
    "y_test = mlb.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-19 12:03:51,411 scripts      INFO     Setting up preliminaries and the statistics arrays\n",
      "2017-12-19 12:03:51,412 scripts      INFO     Found classes Acetylation, Activation, Binding/association, Carboxylation, Deacetylation, Dephosphorylation, Dissociation, Glycosylation, Inhibition, Methylation, Myristoylation, Phosphorylation, Prenylation, Proteolytic-cleavage, State-change, Sulfation, Sumoylation, Ubiquitination\n"
     ]
    }
   ],
   "source": [
    "# Set up the numpy arrays and dictionarys for statistics etc\n",
    "# -------------------------------------------------------------------- #\n",
    "logger.info(\"Setting up preliminaries and the statistics arrays\")\n",
    "logger.info(\"Found classes {}\".format(', '.join(mlb.classes)))\n",
    "n_classes = len(mlb.classes)\n",
    "seeds = range(42, 42 + n_iter, 1)\n",
    "top_features = {\n",
    "    l: {\n",
    "        i: {\n",
    "            j: [] for j in range(n_splits)\n",
    "        } for i in range(n_iter)\n",
    "    } for l in mlb.classes\n",
    "}\n",
    "params = get_parameter_distribution_for_model(model)\n",
    "\n",
    "binary_scoring_funcs = [\n",
    "    ('Binary F1', f1_score),\n",
    "    ('Precision', precision_score),\n",
    "    ('Recall', recall_score),\n",
    "    ('Specificity', specificity),\n",
    "    ('FDR', fdr_score)\n",
    "]\n",
    "multilabel_scoring_funcs = [\n",
    "    ('Label Ranking Loss', label_ranking_loss),\n",
    "    ('Label Ranking Average Precision',\n",
    "        label_ranking_average_precision_score),\n",
    "    ('Macro (weighted) F1', f1_score),\n",
    "    ('Macro (un-weighted) F1', f1_score)\n",
    "]\n",
    "n_scorers = len(binary_scoring_funcs)\n",
    "n_ml_scorers = len(multilabel_scoring_funcs)\n",
    "\n",
    "# 2: position 0 is for validation, position 1 is for testing\n",
    "binary_statistics = np.zeros((n_classes, 2, n_scorers, n_iter, n_splits))\n",
    "multilabel_statistics = np.zeros((2, n_ml_scorers, n_iter, n_splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_fold(X, y, fold_iter, use_binary, model, hyperparam_iter, params):\n",
    "    logger.info(\"Fitting fold {}.\".format(fold_iter + 1))\n",
    "\n",
    "    # Prepare all training and testing data\n",
    "    vectorizer = CountVectorizer(binary=True if use_binary else False, lowercase=False, stop_words=[':', 'GO'])\n",
    "    X = vectorizer.fit_transform(X)\n",
    "\n",
    "    # Build and fit classifier\n",
    "    model_to_tune = make_classifier(\n",
    "        algorithm=model, \n",
    "        random_state=400\n",
    "    )\n",
    "    base_est = RandomizedSearchCV(\n",
    "        estimator=model_to_tune,\n",
    "        scoring='f1', \n",
    "        error_score=0,\n",
    "        cv=3,\n",
    "        n_iter=hyperparam_iter, \n",
    "        n_jobs=1, \n",
    "        refit=True, \n",
    "        random_state=401, \n",
    "        param_distributions=params,\n",
    "    )\n",
    "    clf = OneVsRestClassifier(\n",
    "        estimator=base_est,\n",
    "        n_jobs=n_jobs,\n",
    "    )\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        try:\n",
    "            clf.fit(X.astype(int), y.astype(int))\n",
    "            return clf, vectorizer, False\n",
    "        except TypeError:\n",
    "            logger.info(\"Error fitting sparse input. Converting to dense input.\")\n",
    "            X = X.todense()\n",
    "            clf.fit(X, y)\n",
    "            return clf, vectorizer, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-19 19:53:49,468 scripts      INFO     Fitting bootstrap iteration 1.\n",
      "2017-12-19 19:53:53,061 scripts      INFO     Fitting fold 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21512, 20877)\n"
     ]
    }
   ],
   "source": [
    "# Begin the main show!\n",
    "# ------------------------------------------------------------------- #\n",
    "if __name__ == '__main__':\n",
    "    for bs_iter in range(n_iter):\n",
    "        logger.info(\"Fitting bootstrap iteration {}.\".format(bs_iter + 1))\n",
    "        cv = IterativeStratifiedKFold(n_splits=n_splits, random_state=seeds[bs_iter])\n",
    "        cv = list(cv.split(X_train, y_train))\n",
    "\n",
    "        # Run each label in parallel.\n",
    "        clfs = []\n",
    "        for fold_iter, (train_idx, _) in enumerate(cv):\n",
    "            result = train_fold(\n",
    "                X=X_train[train_idx], \n",
    "                y=y_train[train_idx],\n",
    "                fold_iter=fold_iter,\n",
    "                use_binary=use_binary, \n",
    "                model=model, \n",
    "                hyperparam_iter=hyperparam_iter,\n",
    "                params=params\n",
    "            )\n",
    "            clfs.append(result)\n",
    "\n",
    "#         for Parallel(n_jobs=n_jobs, backend=\"threading\")(\n",
    "#             delayed(train_fold)(\n",
    "#                 X=X_train[train_idx], \n",
    "#                 y=y_train[train_idx],\n",
    "#                 fold_iter=fold_iter,\n",
    "#                 use_binary=use_binary, \n",
    "#                 model=model, \n",
    "#                 hyperparam_iter=hyperparam_iter,\n",
    "#                 params=params\n",
    "#             ) \n",
    "#             for fold_iter, (train_idx, _) in enumerate(cv)\n",
    "#         )\n",
    "\n",
    "        for fold_iter, ((_, validation_idx), (clf, vectorizer, requires_dense)) in enumerate(zip(cv, clfs)):\n",
    "            logger.info(\"Computing binary performance for fold {}.\".format(fold_iter + 1))\n",
    "            y_valid_f_pred = []\n",
    "            y_test_f_pred = []\n",
    "            y_valid_f_proba = []\n",
    "            y_test_f_proba = []\n",
    "\n",
    "            for clf, (label_idx, label) in zip(clf.estimators_, enumerate(mlb.classes)):\n",
    "                logger.info(\"\\tComputing binary performance for label {}.\".format(label))\n",
    "\n",
    "                X_valid_l = vectorizer.transform(X_train[validation_idx])\n",
    "                y_valid_l = y_train[validation_idx, label_idx]\n",
    "\n",
    "                X_test_l = vectorizer.transform(X_test)\n",
    "                y_test_l = y_test[:, label_idx]\n",
    "\n",
    "                if requires_dense:\n",
    "                    X_valid_l = X_valid_l.todense()\n",
    "                    y_valid_l = y_valid_l.todense()\n",
    "                    X_test_l = X_test_l.todense()\n",
    "                    y_test_l = y_test_l.todense()\n",
    "\n",
    "                # Validation scores in binary and probability format\n",
    "                y_valid_l_pred = clf.predict(X_valid_l)\n",
    "                y_valid_l_proba = clf.predict_proba(X_valid_l)\n",
    "\n",
    "                # Held-out testing scores in binary and probability format\n",
    "                y_test_l_pred = clf.predict(X_test_l)\n",
    "                y_test_l_proba = clf.predict_proba(X_test_l)\n",
    "\n",
    "                # Store these per label results in a list which we will\n",
    "                # later use to stack into a multi-label array.\n",
    "                y_valid_f_pred.append([[x] for x in y_valid_l_pred])\n",
    "                y_valid_f_proba.append([[x[1]] for x in y_valid_l_proba])\n",
    "\n",
    "                y_test_f_pred.append([[x] for x in y_test_l_pred])\n",
    "                y_test_f_proba.append([[x[1]] for x in y_test_l_proba])\n",
    "\n",
    "                # Perform scoring on the validation set and the external testing set.\n",
    "\n",
    "                for func_idx, (func_name, func) in enumerate(binary_scoring_funcs):\n",
    "                    if func_name in ['Specificity', 'FDR']:\n",
    "                        scores_v = func(y_valid_l, y_valid_l_pred)\n",
    "                        scores_t = func(y_test_l, y_test_l_pred)\n",
    "                    else:\n",
    "                        scores_v = func(y_valid_l, y_valid_l_pred, average='binary')\n",
    "                        scores_t = func(y_test_l, y_test_l_pred, average='binary')\n",
    "                    binary_statistics[label_idx, 0, func_idx, bs_iter, fold_iter] = scores_v\n",
    "                    binary_statistics[label_idx, 1, func_idx, bs_iter, fold_iter] = scores_t\n",
    "\n",
    "                # Get the top 20 features for this labels's run.\n",
    "                top_n = top_n_features(\n",
    "                    clf=clf,\n",
    "                    go_dag=go_dag,\n",
    "                    ipr_map=ipr_map,\n",
    "                    pfam_map=pfam_map,\n",
    "                    n=get_top_n,\n",
    "                    absolute=abs_weights,\n",
    "                    vectorizer=vectorizer\n",
    "                )\n",
    "                top_features[label][bs_iter][fold_iter].extend(top_n)\n",
    "\n",
    "            logger.info(\"Computing fold mult-label performance.\")\n",
    "            # True scores in multi-label indicator format\n",
    "            y_valid_f = y_train[validation_idx]\n",
    "            y_test_f = y_test\n",
    "\n",
    "            # Validation scores in multi-label indicator format\n",
    "            y_valid_f_pred = np.hstack(y_valid_f_pred)\n",
    "            y_valid_f_proba = np.hstack(y_valid_f_proba)\n",
    "\n",
    "            # Testing scores in multi-label probability format\n",
    "            y_test_f_pred = np.hstack(y_test_f_pred)\n",
    "            y_test_f_proba = np.hstack(y_test_f_proba)\n",
    "\n",
    "            for func_idx, (func_name, func) in enumerate(multilabel_scoring_funcs):\n",
    "                if func_name == 'Macro (weighted) F1':\n",
    "                    scores_v = func(y_valid_f, y_valid_f_pred, average='weighted')\n",
    "                    scores_t = func(y_test_f, y_test_f_pred, average='weighted')\n",
    "                elif func_name == 'Macro (un-weighted) F1':\n",
    "                    scores_v = func(y_valid_f, y_valid_f_pred, average='macro')\n",
    "                    scores_t = func(y_test_f, y_test_f_pred, average='macro')\n",
    "                elif func_name == 'Label Ranking Average Precision':\n",
    "                    scores_v = func(y_valid_f, y_valid_f_proba)\n",
    "                    scores_t = func(y_test_f, y_test_f_proba)\n",
    "                else:\n",
    "                    scores_v = func(y_valid_f, y_valid_f_pred)\n",
    "                    scores_t = func(y_test_f, y_test_f_pred)\n",
    "\n",
    "                multilabel_statistics[0, func_idx, bs_iter, fold_iter] = scores_v\n",
    "                multilabel_statistics[1, func_idx, bs_iter, fold_iter] = scores_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write out all the statistics to a multi-indexed dataframe\n",
    "# -------------------------------------------------------------------- #\n",
    "logger.info(\"Writing statistics to file.\")\n",
    "\n",
    "# Binary Statistics\n",
    "# -------------------------------------------------------------------- #\n",
    "dim_a_size = len(mlb.classes) * 2 * len(binary_scoring_funcs)\n",
    "dim_b_size = n_iter * n_splits\n",
    "\n",
    "func_names = [n for n, _ in binary_scoring_funcs]\n",
    "iterables = [mlb.classes, [\"validation\", \"holdout\"], func_names]\n",
    "names = ['Labels', 'Condition', 'Metric']\n",
    "tuples = list(product(*iterables))\n",
    "index = pd.MultiIndex.from_tuples(tuples, names=names)\n",
    "\n",
    "names = ['Bootstrap Iteration', 'Fold Iteration']\n",
    "arrays = [\n",
    "    ['B{}'.format(i+1) for i in range(n_iter)],\n",
    "    ['F{}'.format(i+1) for i in range(n_splits)]\n",
    "]\n",
    "tuples = list(product(*arrays))\n",
    "columns = pd.MultiIndex.from_tuples(tuples, names=names)\n",
    "\n",
    "binary_df = pd.DataFrame(\n",
    "    binary_statistics.reshape((dim_a_size, dim_b_size)),\n",
    "    index=index, columns=columns\n",
    ").sort_index()\n",
    "binary_df.to_csv('{}/{}'.format(direc, 'binary_stats.csv'), sep=',')\n",
    "\n",
    "# Multi-label Statistics\n",
    "# -------------------------------------------------------------------- #\n",
    "dim_a_size = 2 * len(multilabel_scoring_funcs)\n",
    "dim_b_size = n_iter * n_splits\n",
    "\n",
    "func_names = [n for n, _ in multilabel_scoring_funcs]\n",
    "iterables = [[\"validation\", \"holdout\"], func_names]\n",
    "names = ['Condition', 'Metric']\n",
    "tuples = list(product(*iterables))\n",
    "index = pd.MultiIndex.from_tuples(tuples, names=names)\n",
    "\n",
    "names = ['Bootstrap Iteration', 'Fold Iteration']\n",
    "arrays = [\n",
    "    ['B{}'.format(i+1) for i in range(n_iter)],\n",
    "    ['F{}'.format(i+1) for i in range(n_splits)]\n",
    "]\n",
    "tuples = list(product(*arrays))\n",
    "columns = pd.MultiIndex.from_tuples(tuples, names=names)\n",
    "\n",
    "multilabel_df = pd.DataFrame(\n",
    "    multilabel_statistics.reshape((dim_a_size, dim_b_size)),\n",
    "    index=index, columns=columns\n",
    ").sort_index()\n",
    "multilabel_df.to_csv(\n",
    "    '{}/{}'.format(direc, 'multilabel_stats.csv'), sep=','\n",
    ")\n",
    "\n",
    "# Top N Features, train/y-array index order\n",
    "# -------------------------------------------------------------------- #\n",
    "logger.info(\"Writing label training order.\")\n",
    "with open(\"{}/{}\".format(direc, \"label_order.csv\"), 'wt') as fp:\n",
    "    fp.write(\",\".join(mlb.classes))\n",
    "\n",
    "logging.info(\"Writing top features to file.\")\n",
    "with open('{}/{}'.format(direc, 'top_features.json'), 'wt') as fp:\n",
    "    json.dump(top_features, fp, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute label similarity heatmaps and label correlation heatmap\n",
    "# -------------------------------------------------------------------- #\n",
    "label_features = {l: set() for l in mlb.classes}\n",
    "for idx, label in enumerate(mlb.classes):\n",
    "    selector = y_train[:, idx] == 1\n",
    "    positive_cases = X_train[selector]\n",
    "    for feature_string in positive_cases:\n",
    "        unique = set(feature_string.split(','))\n",
    "        label_features[label] |= unique\n",
    "        \n",
    "j_v_similarity_matrix = np.zeros((len(mlb.classes), len(mlb.classes)))\n",
    "d_v_similarity_matrix = np.zeros((len(mlb.classes), len(mlb.classes)))\n",
    "for i, class_1 in enumerate(sorted(mlb.classes)):\n",
    "    for j, class_2 in enumerate(sorted(mlb.classes)):\n",
    "        set_1 = label_features[class_1]\n",
    "        set_2 = label_features[class_2]\n",
    "        jaccard = len(set_1 & set_2) / len(set_1 | set_2)\n",
    "        dice = 2 * len(set_1 & set_2) / (len(set_1) + len(set_2))\n",
    "        j_v_similarity_matrix[i, j] = jaccard\n",
    "        d_v_similarity_matrix[i, j] = dice\n",
    "        \n",
    "\n",
    "# Create label correlation matrix and then create a new one\n",
    "# Where the columns and rows are in alphabetical order.\n",
    "label_correlation, _ = sp.stats.spearmanr(y_train)\n",
    "s_label_correlation = np.zeros_like(label_correlation)\n",
    "for i, class_1 in enumerate(sorted(mlb.classes)):\n",
    "    for j, class_2 in enumerate(sorted(mlb.classes)):\n",
    "        index_1 = mlb.classes.index(class_1)\n",
    "        index_2 = mlb.classes.index(class_2)\n",
    "        s_label_correlation[i, j] = label_correlation[index_1, index_2]\n",
    "\n",
    "\n",
    "header = \"Columns: {}\\nRows: {}\".format(\n",
    "    ','.join(sorted(mlb.classes)), ','.join(sorted(mlb.classes))\n",
    ")\n",
    "np.savetxt(\n",
    "    X=j_v_similarity_matrix,\n",
    "    fname='{}/{}'.format(direc, 'j_v_similarity_matrix.csv'),\n",
    "    header=header, delimiter=','\n",
    ")\n",
    "np.savetxt(\n",
    "    X=d_v_similarity_matrix,\n",
    "    fname='{}/{}'.format(direc, 'd_v_similarity_matrix.csv'),\n",
    "    header=header, delimiter=','\n",
    ")\n",
    "np.savetxt(\n",
    "    X=s_label_correlation, fname='{}/{}'.format(direc, 'label_spearmanr.csv'),\n",
    "    header=header, delimiter=','\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute label similarity heatmaps for the holdout set\n",
    "# -------------------------------------------------------------------- #\n",
    "holdout_labels = ('dephosphorylation', 'phosphorylation')\n",
    "holdout_label_features = {l: set() for l in holdout_labels}\n",
    "for idx, label in enumerate(mlb.classes):\n",
    "    if label in holdout_labels:\n",
    "        selector = y_test[:, idx] == 1\n",
    "        positive_cases = X_test[selector]\n",
    "        for feature_string in positive_cases:\n",
    "            unique = set(feature_string.split(','))\n",
    "            holdout_label_features[label] |= unique\n",
    "        \n",
    "j_t_similarity_matrix = np.zeros((2, len(mlb.classes)))\n",
    "d_t_similarity_matrix = np.zeros((2, len(mlb.classes)))\n",
    "for i, class_1 in enumerate(sorted(holdout_labels)):\n",
    "    for j, class_2 in enumerate(sorted(mlb.classes)):\n",
    "        set_1 = holdout_label_features[class_1]\n",
    "        set_2 = label_features[class_2]\n",
    "        jaccard = len(set_1 & set_2) / len(set_1 | set_2)\n",
    "        dice = 2 * len(set_1 & set_2) / (len(set_1) + len(set_2))\n",
    "        j_t_similarity_matrix[i, j] = jaccard\n",
    "        d_t_similarity_matrix[i, j] = dice\n",
    "        \n",
    "header = \"Columns: {}\\nRows: {}\".format(\n",
    "    ','.join(sorted(mlb.classes)), ','.join(sorted(holdout_labels))\n",
    ")\n",
    "np.savetxt(\n",
    "    X=j_t_similarity_matrix, \n",
    "    fname='{}/{}'.format(direc, 'j_t_similarity_matrix.csv'), \n",
    "    header=header, delimiter=','\n",
    ")\n",
    "np.savetxt(\n",
    "    X=d_t_similarity_matrix, \n",
    "    fname='{}/{}'.format(direc, 'd_t_similarity_matrix.csv'), \n",
    "    header=header, delimiter=','\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
